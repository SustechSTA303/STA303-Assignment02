{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92caa791",
   "metadata": {},
   "source": [
    "# CLIP zero-shot prediction\n",
    "## Basic import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4678b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from clip import clip\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import Flowers102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b3f5d",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81aa1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seed\n",
    "# SEED = 1 \n",
    "# NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "# NUM_EPOCHS = 30\n",
    "# EVAL_INTERVAL=1\n",
    "# SAVE_DIR = './log'\n",
    "\n",
    "# # Optimizer\n",
    "# LEARNING_RATE = 1e-1\n",
    "# MOMENTUM = 0.9\n",
    "# STEP=5\n",
    "# GAMMA=0.5\n",
    "\n",
    "# CLIP\n",
    "VISUAL_BACKBONE = 'RN50' # RN50, ViT-B/32, ViT-B/16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ec5ce",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c93ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad02c3f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4305b7",
   "metadata": {},
   "source": [
    "### Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29b169e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model, preprocess= clip.load(name=VISUAL_BACKBONE, device=device, download_root='/clip/')\n",
    "model.to(device)\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5327fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 9144\n",
      "Training dataset size: 6400\n",
      "Testing dataset size: 2744\n",
      "Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "# 定义数据集的根目录\n",
    "root_directory = \"caltech101/101_ObjectCategories\"\n",
    "\n",
    "# 定义转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# 创建数据集实例\n",
    "caltech_dataset = torchvision.datasets.ImageFolder(root=root_directory+'/', transform=transform)\n",
    "\n",
    "# 设置训练和测试数据集的比例\n",
    "train_ratio = 0.7\n",
    "dataset_size = len(caltech_dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# 划分训练和测试数据集\n",
    "train_set, test_set = random_split(caltech_dataset, [train_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=12)\n",
    "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=12)\n",
    "\n",
    "# 打印数据集和加载器的大小\n",
    "print(f\"Total dataset size: {dataset_size}\")\n",
    "print(f\"Training dataset size: {len(train_set)}\")\n",
    "print(f\"Testing dataset size: {len(test_set)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# 定义类别名称\n",
    "class_names = caltech_dataset.classes\n",
    "\n",
    "# 定义数据集名称\n",
    "dataset_name = 'caltech-101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2248d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a photo of a' # you can try different prompt\n",
    "\n",
    "def prompt_encode(prompt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prompt (str): the text prefix before the class\n",
    "\n",
    "    Returns:\n",
    "        text_inputs(torch.Tensor)\n",
    "\n",
    "    \"\"\"\n",
    "    ##################### Write your answer here ##################\n",
    "    # 将文本编码为tensor    \n",
    "    text_inputs= torch.cat([clip.tokenize(f\"{prompt} {c}\") for c in class_names]).to(device)\n",
    "    ###############################################################\n",
    "    return text_inputs\n",
    "\n",
    "# 编码文本提示\n",
    "text_inputs = prompt_encode(prompt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadfe048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, image, text_inputs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (torch.nn.Module): 训练好的机器学习模型\n",
    "        image (torch.Tensor): 输入图像张量\n",
    "        text_inputs (torch.Tensor): 输入文本张量\n",
    "\n",
    "    Returns:\n",
    "        logits (torch.Tensor): 模型的预测logits\n",
    "    \"\"\"\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    logits = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae469fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the zero-shot performance on caltech-101 is 45.30%, visual encoder is RN50.\n"
     ]
    }
   ],
   "source": [
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    ##################### Write your answer here ##################\n",
    "    correct_predictions = 0\n",
    "    for image, class_id in test_dataloader:\n",
    "        image=image.to(device)\n",
    "        class_id=class_id.to(device)\n",
    "        \n",
    "        logits=model_inference(model,image,text_inputs)\n",
    "        \n",
    "        predictions = logits.argmax(dim=-1)\n",
    "#         print(predictions,class_id)\n",
    "        correct_predictions += (predictions == class_id).sum().item()\n",
    "    \n",
    "    val_acc = correct_predictions / len(test_set)\n",
    "    \n",
    "    ###############################################################\n",
    "\n",
    "    print(f\"the zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
