{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c421e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from clip import clip\n",
    "import pickle\n",
    "import torchvision.transforms.functional as Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 80\n",
    "\n",
    "VISUAL_BACKBONE = 'RN50' # RN50, ViT-B/32, ViT-B/16\n",
    "\n",
    "IMAGE_PATH = \"Flickr8k/Images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b90c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba683339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(name=VISUAL_BACKBONE, device=device, download_root='/shareddata/clip/')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6921b40b",
   "metadata": {},
   "source": [
    "## Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bcf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"images_captions.pkl\", 'rb') as file:\n",
    "    images_captions = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe841e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_captions = pd.DataFrame(images_captions, columns=['file_name', 'caption'])\n",
    "images_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dddd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_right_form(captions):\n",
    "    captions = list(captions)\n",
    "    for i in range(len(captions)):\n",
    "        captions[i] = captions[i][0]\n",
    "        \n",
    "    text_inputs = clip.tokenize(captions).to(device)\n",
    "    return text_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8448e1",
   "metadata": {},
   "source": [
    "## Compute the top-k accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_largest_k_numbers(A, k):\n",
    "    sorted_array = sorted(A, reverse=True)\n",
    "    largest_k_numbers = sorted_array[:k]\n",
    "    return largest_k_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = to_right_form(images_captions[\"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ca1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_acc(images_captions, k):\n",
    "    scores = []\n",
    "    i = 0\n",
    "    for img_name in images_captions[\"file_name\"]:\n",
    "        image = preprocess(Image.open(IMAGE_PATH + img_name)).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text)\n",
    "            logits, _ = model(image, text)\n",
    "            similarities = (logits.softmax(dim=-1).cpu().numpy())[0]\n",
    "            top_k_similarities = select_largest_k_numbers(similarities, k)\n",
    "            rank = np.sum(top_k_similarities>=similarities[i])\n",
    "            score = 1-rank/k\n",
    "            scores.append(score)\n",
    "            i+=1\n",
    "    \n",
    "    score = np.mean(scores)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e885cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_acc(images_captions, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8883426f",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7ea43",
   "metadata": {},
   "source": [
    "### An Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f848d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_name = \"1000268201_693b08cb0e.jpg\"\n",
    "sample_image = Image.open(IMAGE_PATH + sample_file_name)\n",
    "sample_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd93daf",
   "metadata": {},
   "source": [
    "### Generate the top-5 captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576256e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = preprocess(sample_image).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(sample_image)\n",
    "    text_features = model.encode_text(text)\n",
    "    logits, _ = model(sample_image, text)\n",
    "    similarities = (logits.softmax(dim=-1).cpu().numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90215b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = images_captions\n",
    "candidates[\"similarities\"] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea34100",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_indices = candidates[\"similarities\"].nlargest(5).index\n",
    "\n",
    "# Getting the corresponding values in column A\n",
    "top_5_captions = candidates.loc[top_5_indices, 'caption'].tolist()\n",
    "top_5_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195c856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
