{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Multi-class Classification \n",
    "In this Assignment, you will train a deep model on the CIFAR10 from the scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 150\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_to_rgb(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "transform_mnist_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(convert_to_rgb), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_mnist_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(convert_to_rgb), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(root='../data', train=True,\n",
    "                                              download=True, transform=transform_mnist_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(root='../data', train=False,\n",
    "                                             download=True, transform=transform_mnist_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = train_set.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 第一层卷积层，3个输入通道（对于RGB图像），32个输出通道，3x3卷积核\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # 第二层卷积层\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # 第三层卷积层\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # 最大池化层\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10) # 10个输出类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过卷积层、批量归一化层、激活函数、池化层和dropout层\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        # 扁平化特征图\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        # 通过dropout层和全连接层\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP, gamma=GAMMA)\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# 定义warm-up学习率调度器\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, multiplier):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (multiplier - 1.0) * epoch / warmup_epochs + 1.0\n",
    "        return 1.0\n",
    "\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 在训练代码中引入warm-up学习率调度器\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "warmup_epochs = NUM_EPOCHS//5\n",
    "multiplier = 10\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: per batch training/testing\n",
    "---\n",
    "\n",
    "Please denfine two function named ``train_batch`` and ``test_batch``. These functions are essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Define the loss function i.e [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "2. Take the image as the input and generate the output using the pre-defined SimpleNet.\n",
    "3. Calculate the loss between the output and the corresponding label using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "    output = model(image)\n",
    "    #print(\"Output size:\", output.size())  # Add this line to check the output size\n",
    "    loss = criterion(output, target)\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def one_hot_encoding(target, num_classes):\n",
    "    return F.one_hot(target, num_classes=num_classes).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, image, target):\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150 Train Loss: 0.0132 Acc: 0.5012\n",
      "f1 score in 0th epoch is 0.7593527674863486\n",
      "Begin test......\n",
      "Test Loss: 0.0085 Acc: 0.7363\n",
      "Epoch: 2/150 Train Loss: 0.0072 Acc: 0.7210\n",
      "f1 score in 1th epoch is 0.7753869560313359\n",
      "Begin test......\n",
      "Test Loss: 0.0057 Acc: 0.7693\n",
      "Epoch: 3/150 Train Loss: 0.0055 Acc: 0.7618\n",
      "f1 score in 2th epoch is 0.7821953403165275\n",
      "Begin test......\n",
      "Test Loss: 0.0047 Acc: 0.7891\n",
      "Epoch: 4/150 Train Loss: 0.0048 Acc: 0.7860\n",
      "f1 score in 3th epoch is 0.7851883051227412\n",
      "Begin test......\n",
      "Test Loss: 0.0042 Acc: 0.8072\n",
      "Epoch: 5/150 Train Loss: 0.0043 Acc: 0.8076\n",
      "f1 score in 4th epoch is 0.8029680493122766\n",
      "Begin test......\n",
      "Test Loss: 0.0038 Acc: 0.8243\n",
      "Epoch: 6/150 Train Loss: 0.0039 Acc: 0.8224\n",
      "f1 score in 5th epoch is 0.7955295927497983\n",
      "Begin test......\n",
      "Test Loss: 0.0035 Acc: 0.8433\n",
      "Epoch: 7/150 Train Loss: 0.0036 Acc: 0.8356\n",
      "f1 score in 6th epoch is 0.8093658000999113\n",
      "Begin test......\n",
      "Test Loss: 0.0033 Acc: 0.8510\n",
      "Epoch: 8/150 Train Loss: 0.0034 Acc: 0.8466\n",
      "f1 score in 7th epoch is 0.813281350999781\n",
      "Begin test......\n",
      "Test Loss: 0.0031 Acc: 0.8589\n",
      "Epoch: 9/150 Train Loss: 0.0032 Acc: 0.8555\n",
      "f1 score in 8th epoch is 0.8161172411222748\n",
      "Begin test......\n",
      "Test Loss: 0.0029 Acc: 0.8652\n",
      "Epoch: 10/150 Train Loss: 0.0030 Acc: 0.8605\n",
      "f1 score in 9th epoch is 0.8115294888961572\n",
      "Begin test......\n",
      "Test Loss: 0.0028 Acc: 0.8721\n",
      "Epoch: 11/150 Train Loss: 0.0029 Acc: 0.8670\n",
      "f1 score in 10th epoch is 0.8186114132625448\n",
      "Begin test......\n",
      "Test Loss: 0.0027 Acc: 0.8777\n",
      "Epoch: 12/150 Train Loss: 0.0028 Acc: 0.8720\n",
      "f1 score in 11th epoch is 0.823198023357586\n",
      "Begin test......\n",
      "Test Loss: 0.0026 Acc: 0.8804\n",
      "Epoch: 13/150 Train Loss: 0.0027 Acc: 0.8767\n",
      "f1 score in 12th epoch is 0.8264385021002315\n",
      "Begin test......\n",
      "Test Loss: 0.0026 Acc: 0.8799\n",
      "Epoch: 14/150 Train Loss: 0.0026 Acc: 0.8806\n",
      "f1 score in 13th epoch is 0.8307047850355952\n",
      "Begin test......\n",
      "Test Loss: 0.0025 Acc: 0.8847\n",
      "Epoch: 15/150 Train Loss: 0.0025 Acc: 0.8840\n",
      "f1 score in 14th epoch is 0.8293897568373164\n",
      "Begin test......\n",
      "Test Loss: 0.0025 Acc: 0.8862\n",
      "Epoch: 16/150 Train Loss: 0.0024 Acc: 0.8873\n",
      "f1 score in 15th epoch is 0.8323157228319918\n",
      "Begin test......\n",
      "Test Loss: 0.0024 Acc: 0.8884\n",
      "Epoch: 17/150 Train Loss: 0.0024 Acc: 0.8897\n",
      "f1 score in 16th epoch is 0.8372474880064746\n",
      "Begin test......\n",
      "Test Loss: 0.0023 Acc: 0.8971\n",
      "Epoch: 18/150 Train Loss: 0.0023 Acc: 0.8922\n",
      "f1 score in 17th epoch is 0.8394658200105032\n",
      "Begin test......\n",
      "Test Loss: 0.0023 Acc: 0.8953\n",
      "Epoch: 19/150 Train Loss: 0.0022 Acc: 0.8964\n",
      "f1 score in 18th epoch is 0.8453542948099642\n",
      "Begin test......\n",
      "Test Loss: 0.0022 Acc: 0.8950\n",
      "Epoch: 20/150 Train Loss: 0.0022 Acc: 0.9003\n",
      "f1 score in 19th epoch is 0.8483300851235455\n",
      "Begin test......\n",
      "Test Loss: 0.0022 Acc: 0.9000\n",
      "Epoch: 21/150 Train Loss: 0.0021 Acc: 0.9011\n",
      "f1 score in 20th epoch is 0.8496000156167816\n",
      "Begin test......\n",
      "Test Loss: 0.0022 Acc: 0.9028\n",
      "Epoch: 22/150 Train Loss: 0.0021 Acc: 0.9025\n",
      "f1 score in 21th epoch is 0.8521847833593208\n",
      "Begin test......\n",
      "Test Loss: 0.0021 Acc: 0.9024\n",
      "Epoch: 23/150 Train Loss: 0.0020 Acc: 0.9058\n",
      "f1 score in 22th epoch is 0.8544675607869443\n",
      "Begin test......\n",
      "Test Loss: 0.0022 Acc: 0.8980\n",
      "Epoch: 24/150 Train Loss: 0.0020 Acc: 0.9067\n",
      "f1 score in 23th epoch is 0.8549819607802537\n",
      "Begin test......\n",
      "Test Loss: 0.0021 Acc: 0.9021\n",
      "Epoch: 25/150 Train Loss: 0.0020 Acc: 0.9091\n",
      "f1 score in 24th epoch is 0.8546076870527063\n",
      "Begin test......\n",
      "Test Loss: 0.0021 Acc: 0.9061\n",
      "Epoch: 26/150 Train Loss: 0.0019 Acc: 0.9106\n",
      "f1 score in 25th epoch is 0.8589397119533936\n",
      "Begin test......\n",
      "Test Loss: 0.0021 Acc: 0.9041\n",
      "Epoch: 27/150 Train Loss: 0.0019 Acc: 0.9111\n",
      "f1 score in 26th epoch is 0.861042848418571\n",
      "Begin test......\n",
      "Test Loss: 0.0020 Acc: 0.9076\n",
      "Epoch: 28/150 Train Loss: 0.0018 Acc: 0.9142\n",
      "f1 score in 27th epoch is 0.8624793125219548\n",
      "Begin test......\n",
      "Test Loss: 0.0020 Acc: 0.9102\n",
      "Epoch: 29/150 Train Loss: 0.0018 Acc: 0.9174\n",
      "f1 score in 28th epoch is 0.8654498135750042\n",
      "Begin test......\n",
      "Test Loss: 0.0021 Acc: 0.9041\n",
      "Epoch: 30/150 Train Loss: 0.0018 Acc: 0.9172\n",
      "f1 score in 29th epoch is 0.867129261976279\n",
      "Begin test......\n",
      "Test Loss: 0.0020 Acc: 0.9067\n",
      "Epoch: 31/150 Train Loss: 0.0016 Acc: 0.9244\n",
      "f1 score in 30th epoch is 0.8697980640898748\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9122\n",
      "Epoch: 32/150 Train Loss: 0.0016 Acc: 0.9245\n",
      "f1 score in 31th epoch is 0.8712387971243308\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9132\n",
      "Epoch: 33/150 Train Loss: 0.0016 Acc: 0.9241\n",
      "f1 score in 32th epoch is 0.8723278427762292\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9141\n",
      "Epoch: 34/150 Train Loss: 0.0016 Acc: 0.9255\n",
      "f1 score in 33th epoch is 0.8727489972501584\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9129\n",
      "Epoch: 35/150 Train Loss: 0.0016 Acc: 0.9274\n",
      "f1 score in 34th epoch is 0.8745769436932367\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9128\n",
      "Epoch: 36/150 Train Loss: 0.0016 Acc: 0.9272\n",
      "f1 score in 35th epoch is 0.8758275394337421\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9130\n",
      "Epoch: 37/150 Train Loss: 0.0016 Acc: 0.9272\n",
      "f1 score in 36th epoch is 0.87721808273174\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9139\n",
      "Epoch: 38/150 Train Loss: 0.0016 Acc: 0.9265\n",
      "f1 score in 37th epoch is 0.8785108704400346\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9138\n",
      "Epoch: 39/150 Train Loss: 0.0016 Acc: 0.9273\n",
      "f1 score in 38th epoch is 0.8794688342860242\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9133\n",
      "Epoch: 40/150 Train Loss: 0.0016 Acc: 0.9278\n",
      "f1 score in 39th epoch is 0.8803622202214096\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9153\n",
      "Epoch: 41/150 Train Loss: 0.0016 Acc: 0.9278\n",
      "f1 score in 40th epoch is 0.8824858010299829\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9152\n",
      "Epoch: 42/150 Train Loss: 0.0016 Acc: 0.9280\n",
      "f1 score in 41th epoch is 0.8836024729356299\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9136\n",
      "Epoch: 43/150 Train Loss: 0.0015 Acc: 0.9291\n",
      "f1 score in 42th epoch is 0.8850867424172704\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9148\n",
      "Epoch: 44/150 Train Loss: 0.0016 Acc: 0.9275\n",
      "f1 score in 43th epoch is 0.8870096875559023\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9162\n",
      "Epoch: 45/150 Train Loss: 0.0015 Acc: 0.9299\n",
      "f1 score in 44th epoch is 0.8864672898772604\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9141\n",
      "Epoch: 46/150 Train Loss: 0.0015 Acc: 0.9294\n",
      "f1 score in 45th epoch is 0.8873097043720881\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9149\n",
      "Epoch: 47/150 Train Loss: 0.0015 Acc: 0.9301\n",
      "f1 score in 46th epoch is 0.8879395076109573\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9147\n",
      "Epoch: 48/150 Train Loss: 0.0015 Acc: 0.9300\n",
      "f1 score in 47th epoch is 0.8880448929796325\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9160\n",
      "Epoch: 49/150 Train Loss: 0.0015 Acc: 0.9300\n",
      "f1 score in 48th epoch is 0.8890557437434005\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9151\n",
      "Epoch: 50/150 Train Loss: 0.0015 Acc: 0.9308\n",
      "f1 score in 49th epoch is 0.8895855494228541\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9133\n",
      "Epoch: 51/150 Train Loss: 0.0015 Acc: 0.9304\n",
      "f1 score in 50th epoch is 0.8909466999762694\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9146\n",
      "Epoch: 52/150 Train Loss: 0.0015 Acc: 0.9315\n",
      "f1 score in 51th epoch is 0.8906619168805694\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9152\n",
      "Epoch: 53/150 Train Loss: 0.0015 Acc: 0.9303\n",
      "f1 score in 52th epoch is 0.8921284692523412\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9144\n",
      "Epoch: 54/150 Train Loss: 0.0015 Acc: 0.9304\n",
      "f1 score in 53th epoch is 0.8920113387903211\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9148\n",
      "Epoch: 55/150 Train Loss: 0.0015 Acc: 0.9317\n",
      "f1 score in 54th epoch is 0.8930221415743952\n",
      "Begin test......\n",
      "Test Loss: 0.0019 Acc: 0.9151\n",
      "Epoch: 56/150 Train Loss: 0.0015 Acc: 0.9304\n",
      "f1 score in 55th epoch is 0.8943529853570834\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9156\n",
      "Epoch: 57/150 Train Loss: 0.0015 Acc: 0.9318\n",
      "f1 score in 56th epoch is 0.8952777949681383\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9159\n",
      "Epoch: 58/150 Train Loss: 0.0015 Acc: 0.9315\n",
      "f1 score in 57th epoch is 0.8963870863749583\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9170\n",
      "Epoch: 59/150 Train Loss: 0.0015 Acc: 0.9302\n",
      "f1 score in 58th epoch is 0.8970980085418843\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9168\n",
      "Epoch: 60/150 Train Loss: 0.0015 Acc: 0.9317\n",
      "f1 score in 59th epoch is 0.8979193804504024\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9168\n",
      "Epoch: 61/150 Train Loss: 0.0015 Acc: 0.9324\n",
      "f1 score in 60th epoch is 0.898717000315186\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9171\n",
      "Epoch: 62/150 Train Loss: 0.0014 Acc: 0.9337\n",
      "f1 score in 61th epoch is 0.8998871110772176\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9162\n",
      "Epoch: 63/150 Train Loss: 0.0014 Acc: 0.9329\n",
      "f1 score in 62th epoch is 0.9006111447201652\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9164\n",
      "Epoch: 64/150 Train Loss: 0.0014 Acc: 0.9340\n",
      "f1 score in 63th epoch is 0.9006643837559447\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9168\n",
      "Epoch: 65/150 Train Loss: 0.0014 Acc: 0.9337\n",
      "f1 score in 64th epoch is 0.9016950775036737\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9159\n",
      "Epoch: 66/150 Train Loss: 0.0014 Acc: 0.9324\n",
      "f1 score in 65th epoch is 0.9014610124217436\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9173\n",
      "Epoch: 67/150 Train Loss: 0.0014 Acc: 0.9342\n",
      "f1 score in 66th epoch is 0.9016959827364712\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9181\n",
      "Epoch: 68/150 Train Loss: 0.0014 Acc: 0.9349\n",
      "f1 score in 67th epoch is 0.9019027345791693\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9156\n",
      "Epoch: 69/150 Train Loss: 0.0014 Acc: 0.9339\n",
      "f1 score in 68th epoch is 0.9022660899743054\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9166\n",
      "Epoch: 70/150 Train Loss: 0.0014 Acc: 0.9343\n",
      "f1 score in 69th epoch is 0.9029032182217039\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9155\n",
      "Epoch: 71/150 Train Loss: 0.0014 Acc: 0.9341\n",
      "f1 score in 70th epoch is 0.9035500704683396\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9171\n",
      "Epoch: 72/150 Train Loss: 0.0014 Acc: 0.9344\n",
      "f1 score in 71th epoch is 0.9038706974399143\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9170\n",
      "Epoch: 73/150 Train Loss: 0.0014 Acc: 0.9353\n",
      "f1 score in 72th epoch is 0.9042839609944755\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9164\n",
      "Epoch: 74/150 Train Loss: 0.0014 Acc: 0.9349\n",
      "f1 score in 73th epoch is 0.9035636562021633\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9173\n",
      "Epoch: 75/150 Train Loss: 0.0014 Acc: 0.9352\n",
      "f1 score in 74th epoch is 0.9039960393835187\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9174\n",
      "Epoch: 76/150 Train Loss: 0.0014 Acc: 0.9360\n",
      "f1 score in 75th epoch is 0.9042718750538948\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9177\n",
      "Epoch: 77/150 Train Loss: 0.0014 Acc: 0.9355\n",
      "f1 score in 76th epoch is 0.9047130569099262\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9183\n",
      "Epoch: 78/150 Train Loss: 0.0014 Acc: 0.9353\n",
      "f1 score in 77th epoch is 0.9051474375801086\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9184\n",
      "Epoch: 79/150 Train Loss: 0.0014 Acc: 0.9345\n",
      "f1 score in 78th epoch is 0.9054208056328389\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9188\n",
      "Epoch: 80/150 Train Loss: 0.0014 Acc: 0.9364\n",
      "f1 score in 79th epoch is 0.9055677176346113\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9179\n",
      "Epoch: 81/150 Train Loss: 0.0014 Acc: 0.9367\n",
      "f1 score in 80th epoch is 0.9059609010840406\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9191\n",
      "Epoch: 82/150 Train Loss: 0.0014 Acc: 0.9364\n",
      "f1 score in 81th epoch is 0.9062101210624444\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9178\n",
      "Epoch: 83/150 Train Loss: 0.0014 Acc: 0.9369\n",
      "f1 score in 82th epoch is 0.9064467817640058\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9188\n",
      "Epoch: 84/150 Train Loss: 0.0014 Acc: 0.9367\n",
      "f1 score in 83th epoch is 0.9067101237415028\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9189\n",
      "Epoch: 85/150 Train Loss: 0.0014 Acc: 0.9371\n",
      "f1 score in 84th epoch is 0.9070447865031194\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9182\n",
      "Epoch: 86/150 Train Loss: 0.0014 Acc: 0.9384\n",
      "f1 score in 85th epoch is 0.9067625854168359\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9180\n",
      "Epoch: 87/150 Train Loss: 0.0014 Acc: 0.9367\n",
      "f1 score in 86th epoch is 0.9076051996644227\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9185\n",
      "Epoch: 88/150 Train Loss: 0.0014 Acc: 0.9378\n",
      "f1 score in 87th epoch is 0.9082134734611235\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9187\n",
      "Epoch: 89/150 Train Loss: 0.0013 Acc: 0.9376\n",
      "f1 score in 88th epoch is 0.9085380612872066\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9180\n",
      "Epoch: 90/150 Train Loss: 0.0013 Acc: 0.9379\n",
      "f1 score in 89th epoch is 0.9085059020917797\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9177\n",
      "Epoch: 91/150 Train Loss: 0.0014 Acc: 0.9384\n",
      "f1 score in 90th epoch is 0.90928469447976\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9187\n",
      "Epoch: 92/150 Train Loss: 0.0013 Acc: 0.9385\n",
      "f1 score in 91th epoch is 0.9092638846990816\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9189\n",
      "Epoch: 93/150 Train Loss: 0.0013 Acc: 0.9395\n",
      "f1 score in 92th epoch is 0.9099118814772753\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9182\n",
      "Epoch: 94/150 Train Loss: 0.0013 Acc: 0.9385\n",
      "f1 score in 93th epoch is 0.9101932529345469\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9189\n",
      "Epoch: 95/150 Train Loss: 0.0013 Acc: 0.9386\n",
      "f1 score in 94th epoch is 0.9107121574416348\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9186\n",
      "Epoch: 96/150 Train Loss: 0.0013 Acc: 0.9382\n",
      "f1 score in 95th epoch is 0.9111034731781432\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9165\n",
      "Epoch: 97/150 Train Loss: 0.0013 Acc: 0.9381\n",
      "f1 score in 96th epoch is 0.9114786331925216\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9189\n",
      "Epoch: 98/150 Train Loss: 0.0013 Acc: 0.9401\n",
      "f1 score in 97th epoch is 0.9120559033360485\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9175\n",
      "Epoch: 99/150 Train Loss: 0.0013 Acc: 0.9393\n",
      "f1 score in 98th epoch is 0.9121250778783606\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9181\n",
      "Epoch: 107/150 Train Loss: 0.0013 Acc: 0.9416\n",
      "f1 score in 106th epoch is 0.9138265387103046\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9181\n",
      "Epoch: 108/150 Train Loss: 0.0013 Acc: 0.9415\n",
      "f1 score in 107th epoch is 0.913667077144606\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9193\n",
      "Epoch: 109/150 Train Loss: 0.0013 Acc: 0.9411\n",
      "f1 score in 108th epoch is 0.9134857146368035\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9195\n",
      "Epoch: 110/150 Train Loss: 0.0013 Acc: 0.9411\n",
      "f1 score in 109th epoch is 0.9133095476344298\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9191\n",
      "Epoch: 111/150 Train Loss: 0.0013 Acc: 0.9414\n",
      "f1 score in 110th epoch is 0.9136122459969265\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9189\n",
      "Epoch: 112/150 Train Loss: 0.0013 Acc: 0.9411\n",
      "f1 score in 111th epoch is 0.9140208725913562\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9185\n",
      "Epoch: 113/150 Train Loss: 0.0013 Acc: 0.9410\n",
      "f1 score in 112th epoch is 0.9142195349435237\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9206\n",
      "Epoch: 114/150 Train Loss: 0.0013 Acc: 0.9419\n",
      "f1 score in 113th epoch is 0.9144425225134981\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9203\n",
      "Epoch: 115/150 Train Loss: 0.0013 Acc: 0.9419\n",
      "f1 score in 114th epoch is 0.9147357131737726\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9190\n",
      "Epoch: 116/150 Train Loss: 0.0013 Acc: 0.9426\n",
      "f1 score in 115th epoch is 0.9152060423433087\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9171\n",
      "Epoch: 117/150 Train Loss: 0.0012 Acc: 0.9421\n",
      "f1 score in 116th epoch is 0.9154121936303513\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9202\n",
      "Epoch: 118/150 Train Loss: 0.0012 Acc: 0.9430\n",
      "f1 score in 117th epoch is 0.9155957232638555\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9199\n",
      "Epoch: 119/150 Train Loss: 0.0013 Acc: 0.9414\n",
      "f1 score in 118th epoch is 0.9157891462813404\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9197\n",
      "Epoch: 120/150 Train Loss: 0.0012 Acc: 0.9433\n",
      "f1 score in 119th epoch is 0.9158766346088084\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9194\n",
      "Epoch: 121/150 Train Loss: 0.0012 Acc: 0.9429\n",
      "f1 score in 120th epoch is 0.9159844840324342\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9204\n",
      "Epoch: 122/150 Train Loss: 0.0012 Acc: 0.9434\n",
      "f1 score in 121th epoch is 0.9160760217455072\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9201\n",
      "Epoch: 123/150 Train Loss: 0.0012 Acc: 0.9435\n",
      "f1 score in 122th epoch is 0.9160782771592599\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9214\n",
      "Epoch: 124/150 Train Loss: 0.0012 Acc: 0.9438\n",
      "f1 score in 123th epoch is 0.916172805455803\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9206\n",
      "Epoch: 125/150 Train Loss: 0.0012 Acc: 0.9432\n",
      "f1 score in 124th epoch is 0.916417190439756\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9202\n",
      "Epoch: 126/150 Train Loss: 0.0012 Acc: 0.9435\n",
      "f1 score in 125th epoch is 0.9167357859667131\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9202\n",
      "Epoch: 127/150 Train Loss: 0.0012 Acc: 0.9440\n",
      "f1 score in 126th epoch is 0.9170816404433174\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9207\n",
      "Epoch: 128/150 Train Loss: 0.0012 Acc: 0.9448\n",
      "f1 score in 127th epoch is 0.9174045613131536\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9186\n",
      "Epoch: 129/150 Train Loss: 0.0012 Acc: 0.9432\n",
      "f1 score in 128th epoch is 0.9173885564577731\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9193\n",
      "Epoch: 130/150 Train Loss: 0.0012 Acc: 0.9433\n",
      "f1 score in 129th epoch is 0.9176885938971918\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9196\n",
      "Epoch: 131/150 Train Loss: 0.0012 Acc: 0.9448\n",
      "f1 score in 130th epoch is 0.9178418459358576\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9190\n",
      "Epoch: 132/150 Train Loss: 0.0012 Acc: 0.9443\n",
      "f1 score in 131th epoch is 0.917999596897416\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9207\n",
      "Epoch: 133/150 Train Loss: 0.0012 Acc: 0.9448\n",
      "f1 score in 132th epoch is 0.9182316221315177\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9194\n",
      "Epoch: 134/150 Train Loss: 0.0012 Acc: 0.9453\n",
      "f1 score in 133th epoch is 0.9185380022430505\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9203\n",
      "Epoch: 135/150 Train Loss: 0.0012 Acc: 0.9446\n",
      "f1 score in 134th epoch is 0.9187565627032479\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9213\n",
      "Epoch: 136/150 Train Loss: 0.0012 Acc: 0.9445\n",
      "f1 score in 135th epoch is 0.9191941397845971\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9203\n",
      "Epoch: 137/150 Train Loss: 0.0012 Acc: 0.9444\n",
      "f1 score in 136th epoch is 0.9194810928850283\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9208\n",
      "Epoch: 138/150 Train Loss: 0.0012 Acc: 0.9463\n",
      "f1 score in 137th epoch is 0.9195225802794459\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9195\n",
      "Epoch: 139/150 Train Loss: 0.0012 Acc: 0.9456\n",
      "f1 score in 138th epoch is 0.9194908142652111\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9212\n",
      "Epoch: 140/150 Train Loss: 0.0012 Acc: 0.9456\n",
      "f1 score in 139th epoch is 0.919768855212234\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9205\n",
      "Epoch: 141/150 Train Loss: 0.0012 Acc: 0.9458\n",
      "f1 score in 140th epoch is 0.9201277455994022\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9215\n",
      "Epoch: 142/150 Train Loss: 0.0012 Acc: 0.9453\n",
      "f1 score in 141th epoch is 0.9203196898093471\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9186\n",
      "Epoch: 143/150 Train Loss: 0.0012 Acc: 0.9450\n",
      "f1 score in 142th epoch is 0.9204263534753699\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9202\n",
      "Epoch: 144/150 Train Loss: 0.0012 Acc: 0.9469\n",
      "f1 score in 143th epoch is 0.9206049228588837\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9204\n",
      "Epoch: 145/150 Train Loss: 0.0012 Acc: 0.9466\n",
      "f1 score in 144th epoch is 0.9210800732171391\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9205\n",
      "Epoch: 146/150 Train Loss: 0.0012 Acc: 0.9472\n",
      "f1 score in 145th epoch is 0.9214717008127201\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9188\n",
      "Epoch: 147/150 Train Loss: 0.0012 Acc: 0.9468\n",
      "f1 score in 146th epoch is 0.9218618131497378\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9210\n",
      "Epoch: 148/150 Train Loss: 0.0011 Acc: 0.9471\n",
      "f1 score in 147th epoch is 0.922171763521632\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9200\n",
      "Epoch: 149/150 Train Loss: 0.0011 Acc: 0.9469\n",
      "f1 score in 148th epoch is 0.9223616907416883\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9209\n",
      "Epoch: 150/150 Train Loss: 0.0011 Acc: 0.9471\n",
      "f1 score in 149th epoch is 0.922397890685838\n",
      "Begin test......\n",
      "Test Loss: 0.0018 Acc: 0.9204\n",
      "训练模型用时：5126.532082796097秒\n"
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "start_time = time.time()  # 记录开始时间\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "# 日志文件\n",
    "log_file = 'training_log.txt'\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        #########################\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    #########\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_targets.extend(target.cpu().numpy())\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    print(f\"f1 score in {epoch}th epoch is {f1}\")\n",
    "\n",
    "############\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "    # change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    \n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            ##################\n",
    "          #  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "          #  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "            #######################\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{NUM_EPOCHS}\\n')\n",
    "        f.write(f'Train Accuracy: {epoch_acc:.4f}%\\n')\n",
    "        f.write(f'Test Accuracy: {val_acc:.4f}%\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # save the model in last epoch\n",
    "        if (epoch +1) == NUM_EPOCHS:\n",
    "            \n",
    "            state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': (epoch+1),\n",
    "            }\n",
    "\n",
    "            # check the dir\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "\n",
    "            # save the state\n",
    "            torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch+1))))\n",
    "            \n",
    "end_time = time.time()  # 记录结束时间\n",
    "duration = end_time - start_time  # 计算训练时间\n",
    "print(f\"训练模型用时：{duration}秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Instance inference\n",
    "---\n",
    "The task is to visualizes an image along with model prediction and class probabilities.\n",
    "\n",
    "**To do**: \n",
    "1. Calculate the prediction and the probabilities for each class.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(test_dataloader))\n",
    "input = inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# input: image, model\n",
    "# outputs: predict_label, probabilities\n",
    "# predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "###############################################################\n",
    "input_tensor = input.unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    predict_label = torch.argmax(probabilities)\n",
    "# predict_label = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print probabilities for each class:\n",
      "T-shirt/top: 0.0000\n",
      "Trouser: 0.0000\n",
      "Pullover: 0.0000\n",
      "Dress: 0.0000\n",
      "Coat: 0.0000\n",
      "Sandal: 0.0002\n",
      "Shirt: 0.0000\n",
      "Sneaker: 0.0003\n",
      "Bag: 0.0000\n",
      "Ankle boot: 0.9996\n"
     ]
    }
   ],
   "source": [
    "predicted_class = class_names[predict_label.item()]\n",
    "predicted_probability = probabilities[0][predict_label.item()]\n",
    "image = input.numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image)\n",
    "plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}', \n",
    "            color='white', backgroundcolor='black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Print probabilities for each class\n",
    "print('Print probabilities for each class:')\n",
    "for i in range(len(class_names)):\n",
    "    print(f'{class_names[i]}: {probabilities[0][i].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50120,0.72098,0.76178,0.78597,0.80760,0.82242,0.83563,0.84658,0.85547,0.86053,0.86700,0.87195,0.87667,0.88062,0.88402,0.88728,0.88973,0.89217,0.89637,0.90028,0.90105,0.90255,0.90575,0.90673,0.90908,0.91058,0.91108,0.91422,0.91737,0.91723,0.92443,0.92452,0.92412,0.92547,0.92735,0.92720,0.92722,0.92655,0.92730,0.92777,0.92778,0.92798,0.92905,0.92753,0.92987,0.92943,0.93013,0.92998,0.92998,0.93078,0.93042,0.93147,0.93032,0.93043,0.93172,0.93040,0.93182,0.93145,0.93020,0.93170,0.93238,0.93372,0.93290,0.93403,0.93375,0.93242,0.93422,0.93495,0.93395,0.93433,0.93413,0.93437,0.93527,0.93495,0.93522,0.93598,0.93555,0.93532,0.93447,0.93637,0.93668,0.93640,0.93688,0.93667,0.93712,0.93835,0.93670,0.93777,0.93765,0.93793,0.93840,0.93847,0.93950,0.93850,0.93863,0.93823,0.93805,0.94007,0.93928,0.93950,0.93940,0.93833,0.93962,0.94003,0.94073,0.94068,0.94165,0.94148,0.94112,0.94110,0.94143,0.94108,0.94098,0.94193,0.94195,0.94263,0.94212,0.94300,0.94135,0.94333,0.94288,0.94335,0.94353,0.94380,0.94320,0.94348,0.94397,0.94480,0.94323,0.94325,0.94483,0.94432,0.94480,0.94532,0.94455,0.94445,0.94443,0.94628,0.94563,0.94557,0.94583,0.94530,0.94500,0.94687,0.94655,0.94718,0.94682,0.94708,0.94693,0.94710,\n",
      "\n",
      "\n",
      "0.73630,0.76930,0.78910,0.80720,0.82430,0.84330,0.85100,0.85890,0.86520,0.87210,0.87770,0.88040,0.87990,0.88470,0.88620,0.88840,0.89710,0.89530,0.89500,0.90000,0.90280,0.90240,0.89800,0.90210,0.90610,0.90410,0.90760,0.91020,0.90410,0.90670,0.91220,0.91320,0.91410,0.91290,0.91280,0.91300,0.91390,0.91380,0.91330,0.91530,0.91520,0.91360,0.91480,0.91620,0.91410,0.91490,0.91470,0.91600,0.91510,0.91330,0.91460,0.91520,0.91440,0.91480,0.91510,0.91560,0.91590,0.91700,0.91680,0.91680,0.91710,0.91620,0.91640,0.91680,0.91590,0.91730,0.91810,0.91560,0.91660,0.91550,0.91710,0.91700,0.91640,0.91730,0.91740,0.91770,0.91830,0.91840,0.91880,0.91790,0.91910,0.91780,0.91880,0.91890,0.91820,0.91800,0.91850,0.91870,0.91800,0.91770,0.91870,0.91890,0.91820,0.91890,0.91860,0.91650,0.91890,0.91750,0.91810,0.91750,0.91830,0.91870,0.91900,0.91900,0.92000,0.91860,0.91810,0.91930,0.91950,0.91910,0.91890,0.91850,0.92060,0.92030,0.91900,0.91710,0.92020,0.91990,0.91970,0.91940,0.92040,0.92010,0.92140,0.92060,0.92020,0.92020,0.92070,0.91860,0.91930,0.91960,0.91900,0.92070,0.91940,0.92030,0.92130,0.92030,0.92080,0.91950,0.92120,0.92050,0.92150,0.91860,0.92020,0.92040,0.92050,0.91880,0.92100,0.92000,0.92090,0.92040,"
     ]
    }
   ],
   "source": [
    "for i in np.array(training_acc):\n",
    "    print(f\"{i:.5f}\",end=',')\n",
    "print(\"\\n\\n\")\n",
    "for i in np.array(testing_acc):\n",
    "    print(f\"{i:.5f}\",end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (300,) and (150,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m301\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m line1 \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_acc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain acc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m line2 \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mplot(epochs, np\u001b[38;5;241m.\u001b[39marray(testing_acc), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest acc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (300,) and (150,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "epochs = list(range(1, 301))\n",
    "line1 = ax.plot(epochs, np.array(training_acc), label=\"train acc\")\n",
    "line2 = ax.plot(epochs, np.array(testing_acc), label=\"test acc\")\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.grid(True)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
