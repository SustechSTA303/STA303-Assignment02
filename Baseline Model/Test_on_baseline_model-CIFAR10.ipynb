{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Multi-class Classification \n",
    "In this Assignment, you will train a deep model on the CIFAR10 from the scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 150\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar10 transform\n",
    "transform_cifar10_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "                                        download=True, transform=transform_cifar10_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "                                       download=True, transform=transform_cifar10_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = train_set.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu3(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        x = self.relu4(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (relu4): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP, gamma=GAMMA)\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# 定义warm-up学习率调度器\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, multiplier):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (multiplier - 1.0) * epoch / warmup_epochs + 1.0\n",
    "        return 1.0\n",
    "\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 在训练代码中引入warm-up学习率调度器\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "warmup_epochs = NUM_EPOCHS//5\n",
    "multiplier = 10\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: per batch training/testing\n",
    "---\n",
    "\n",
    "Please denfine two function named ``train_batch`` and ``test_batch``. These functions are essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Define the loss function i.e [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "2. Take the image as the input and generate the output using the pre-defined SimpleNet.\n",
    "3. Calculate the loss between the output and the corresponding label using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def one_hot_encoding(target, num_classes):\n",
    "    return F.one_hot(target, num_classes=num_classes).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, image, target):\n",
    "\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "        \n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150 Train Loss: 0.0339 Acc: 0.2084\n",
      "f1 score in 0th epoch is 0.15416666666666665\n",
      "Begin test......\n",
      "Test Loss: 0.0309 Acc: 0.3442\n",
      "Epoch: 2/150 Train Loss: 0.0299 Acc: 0.3037\n",
      "f1 score in 1th epoch is 0.34077380952380953\n",
      "Begin test......\n",
      "Test Loss: 0.0274 Acc: 0.4077\n",
      "Epoch: 3/150 Train Loss: 0.0272 Acc: 0.3618\n",
      "f1 score in 2th epoch is 0.32154528478057887\n",
      "Begin test......\n",
      "Test Loss: 0.0248 Acc: 0.4509\n",
      "Epoch: 4/150 Train Loss: 0.0253 Acc: 0.4071\n",
      "f1 score in 3th epoch is 0.31261101031321614\n",
      "Begin test......\n",
      "Test Loss: 0.0232 Acc: 0.4821\n",
      "Epoch: 5/150 Train Loss: 0.0239 Acc: 0.4448\n",
      "f1 score in 4th epoch is 0.30334058302808303\n",
      "Begin test......\n",
      "Test Loss: 0.0219 Acc: 0.5088\n",
      "Epoch: 6/150 Train Loss: 0.0227 Acc: 0.4728\n",
      "f1 score in 5th epoch is 0.3556777220457561\n",
      "Begin test......\n",
      "Test Loss: 0.0207 Acc: 0.5304\n",
      "Epoch: 7/150 Train Loss: 0.0216 Acc: 0.5029\n",
      "f1 score in 6th epoch is 0.3766773273682789\n",
      "Begin test......\n",
      "Test Loss: 0.0200 Acc: 0.5509\n",
      "Epoch: 8/150 Train Loss: 0.0207 Acc: 0.5261\n",
      "f1 score in 7th epoch is 0.38697323817535517\n",
      "Begin test......\n",
      "Test Loss: 0.0190 Acc: 0.5686\n",
      "Epoch: 9/150 Train Loss: 0.0198 Acc: 0.5492\n",
      "f1 score in 8th epoch is 0.37011678919357893\n",
      "Begin test......\n",
      "Test Loss: 0.0180 Acc: 0.5937\n",
      "Epoch: 10/150 Train Loss: 0.0188 Acc: 0.5692\n",
      "f1 score in 9th epoch is 0.4036025398282891\n",
      "Begin test......\n",
      "Test Loss: 0.0173 Acc: 0.6154\n",
      "Epoch: 11/150 Train Loss: 0.0181 Acc: 0.5903\n",
      "f1 score in 10th epoch is 0.4188094399469611\n",
      "Begin test......\n",
      "Test Loss: 0.0168 Acc: 0.6247\n",
      "Epoch: 12/150 Train Loss: 0.0173 Acc: 0.6088\n",
      "f1 score in 11th epoch is 0.4331509031446929\n",
      "Begin test......\n",
      "Test Loss: 0.0162 Acc: 0.6357\n",
      "Epoch: 13/150 Train Loss: 0.0166 Acc: 0.6257\n",
      "f1 score in 12th epoch is 0.4397643242050766\n",
      "Begin test......\n",
      "Test Loss: 0.0153 Acc: 0.6553\n",
      "Epoch: 14/150 Train Loss: 0.0161 Acc: 0.6389\n",
      "f1 score in 13th epoch is 0.4569839361560695\n",
      "Begin test......\n",
      "Test Loss: 0.0149 Acc: 0.6669\n",
      "Epoch: 15/150 Train Loss: 0.0155 Acc: 0.6512\n",
      "f1 score in 14th epoch is 0.4806753549105039\n",
      "Begin test......\n",
      "Test Loss: 0.0148 Acc: 0.6672\n",
      "Epoch: 16/150 Train Loss: 0.0150 Acc: 0.6648\n",
      "f1 score in 15th epoch is 0.47305061937784293\n",
      "Begin test......\n",
      "Test Loss: 0.0148 Acc: 0.6730\n",
      "Epoch: 17/150 Train Loss: 0.0146 Acc: 0.6740\n",
      "f1 score in 16th epoch is 0.4730432155947368\n",
      "Begin test......\n",
      "Test Loss: 0.0139 Acc: 0.6853\n",
      "Epoch: 18/150 Train Loss: 0.0141 Acc: 0.6838\n",
      "f1 score in 17th epoch is 0.47426356684684834\n",
      "Begin test......\n",
      "Test Loss: 0.0138 Acc: 0.6877\n",
      "Epoch: 19/150 Train Loss: 0.0138 Acc: 0.6920\n",
      "f1 score in 18th epoch is 0.4950040903264181\n",
      "Begin test......\n",
      "Test Loss: 0.0131 Acc: 0.7052\n",
      "Epoch: 20/150 Train Loss: 0.0134 Acc: 0.7018\n",
      "f1 score in 19th epoch is 0.49896710174615533\n",
      "Begin test......\n",
      "Test Loss: 0.0134 Acc: 0.7042\n",
      "Epoch: 21/150 Train Loss: 0.0131 Acc: 0.7061\n",
      "f1 score in 20th epoch is 0.5169714498661092\n",
      "Begin test......\n",
      "Test Loss: 0.0134 Acc: 0.6980\n",
      "Epoch: 22/150 Train Loss: 0.0128 Acc: 0.7162\n",
      "f1 score in 21th epoch is 0.5272560884379763\n",
      "Begin test......\n",
      "Test Loss: 0.0128 Acc: 0.7126\n",
      "Epoch: 23/150 Train Loss: 0.0125 Acc: 0.7217\n",
      "f1 score in 22th epoch is 0.5402846100017428\n",
      "Begin test......\n",
      "Test Loss: 0.0125 Acc: 0.7189\n",
      "Epoch: 24/150 Train Loss: 0.0122 Acc: 0.7297\n",
      "f1 score in 23th epoch is 0.5513549992583039\n",
      "Begin test......\n",
      "Test Loss: 0.0124 Acc: 0.7246\n",
      "Epoch: 25/150 Train Loss: 0.0119 Acc: 0.7332\n",
      "f1 score in 24th epoch is 0.5516480565232339\n",
      "Begin test......\n",
      "Test Loss: 0.0125 Acc: 0.7225\n",
      "Epoch: 26/150 Train Loss: 0.0116 Acc: 0.7401\n",
      "f1 score in 25th epoch is 0.5596720593028905\n",
      "Begin test......\n",
      "Test Loss: 0.0122 Acc: 0.7290\n",
      "Epoch: 27/150 Train Loss: 0.0114 Acc: 0.7454\n",
      "f1 score in 26th epoch is 0.5662834403149932\n",
      "Begin test......\n",
      "Test Loss: 0.0123 Acc: 0.7292\n",
      "Epoch: 28/150 Train Loss: 0.0111 Acc: 0.7490\n",
      "f1 score in 27th epoch is 0.5737290719849435\n",
      "Begin test......\n",
      "Test Loss: 0.0121 Acc: 0.7315\n",
      "Epoch: 29/150 Train Loss: 0.0110 Acc: 0.7537\n",
      "f1 score in 28th epoch is 0.5802779979479806\n",
      "Begin test......\n",
      "Test Loss: 0.0117 Acc: 0.7427\n",
      "Epoch: 30/150 Train Loss: 0.0107 Acc: 0.7596\n",
      "f1 score in 29th epoch is 0.5875118805217331\n",
      "Begin test......\n",
      "Test Loss: 0.0119 Acc: 0.7368\n",
      "Epoch: 31/150 Train Loss: 0.0095 Acc: 0.7875\n",
      "f1 score in 30th epoch is 0.5903840138669166\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7571\n",
      "Epoch: 32/150 Train Loss: 0.0094 Acc: 0.7921\n",
      "f1 score in 31th epoch is 0.5955522934002417\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7588\n",
      "Epoch: 33/150 Train Loss: 0.0093 Acc: 0.7915\n",
      "f1 score in 32th epoch is 0.6017906108210589\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7587\n",
      "Epoch: 34/150 Train Loss: 0.0092 Acc: 0.7954\n",
      "f1 score in 33th epoch is 0.6098160397778614\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7579\n",
      "Epoch: 35/150 Train Loss: 0.0092 Acc: 0.7965\n",
      "f1 score in 34th epoch is 0.6122060123424446\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7618\n",
      "Epoch: 36/150 Train Loss: 0.0091 Acc: 0.7989\n",
      "f1 score in 35th epoch is 0.6163506929511646\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7588\n",
      "Epoch: 37/150 Train Loss: 0.0091 Acc: 0.7999\n",
      "f1 score in 36th epoch is 0.6220546624183682\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7581\n",
      "Epoch: 38/150 Train Loss: 0.0090 Acc: 0.7993\n",
      "f1 score in 37th epoch is 0.6195879198603308\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7563\n",
      "Epoch: 39/150 Train Loss: 0.0089 Acc: 0.7990\n",
      "f1 score in 38th epoch is 0.6230735414398703\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7592\n",
      "Epoch: 40/150 Train Loss: 0.0089 Acc: 0.8021\n",
      "f1 score in 39th epoch is 0.6295740167333645\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7564\n",
      "Epoch: 41/150 Train Loss: 0.0089 Acc: 0.8036\n",
      "f1 score in 40th epoch is 0.6279432974513511\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7562\n",
      "Epoch: 42/150 Train Loss: 0.0088 Acc: 0.8028\n",
      "f1 score in 41th epoch is 0.6304166636554077\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7586\n",
      "Epoch: 43/150 Train Loss: 0.0088 Acc: 0.8059\n",
      "f1 score in 42th epoch is 0.6316093501883323\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7576\n",
      "Epoch: 44/150 Train Loss: 0.0088 Acc: 0.8042\n",
      "f1 score in 43th epoch is 0.6344594784733434\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7581\n",
      "Epoch: 45/150 Train Loss: 0.0087 Acc: 0.8067\n",
      "f1 score in 44th epoch is 0.6367664326194273\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7603\n",
      "Epoch: 46/150 Train Loss: 0.0087 Acc: 0.8087\n",
      "f1 score in 45th epoch is 0.6446856302253855\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7581\n",
      "Epoch: 47/150 Train Loss: 0.0087 Acc: 0.8052\n",
      "f1 score in 46th epoch is 0.651246560408928\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7597\n",
      "Epoch: 48/150 Train Loss: 0.0086 Acc: 0.8086\n",
      "f1 score in 47th epoch is 0.6570732353662606\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7611\n",
      "Epoch: 49/150 Train Loss: 0.0086 Acc: 0.8080\n",
      "f1 score in 48th epoch is 0.6641569993027026\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7609\n",
      "Epoch: 50/150 Train Loss: 0.0085 Acc: 0.8086\n",
      "f1 score in 49th epoch is 0.6683516992388744\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7616\n",
      "Epoch: 51/150 Train Loss: 0.0085 Acc: 0.8105\n",
      "f1 score in 50th epoch is 0.6724940790699775\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7597\n",
      "Epoch: 52/150 Train Loss: 0.0086 Acc: 0.8104\n",
      "f1 score in 51th epoch is 0.6741197706250445\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7610\n",
      "Epoch: 53/150 Train Loss: 0.0085 Acc: 0.8125\n",
      "f1 score in 52th epoch is 0.6745045790152147\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7588\n",
      "Epoch: 54/150 Train Loss: 0.0084 Acc: 0.8139\n",
      "f1 score in 53th epoch is 0.6756411132539806\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7582\n",
      "Epoch: 55/150 Train Loss: 0.0084 Acc: 0.8116\n",
      "f1 score in 54th epoch is 0.6794134417535402\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7587\n",
      "Epoch: 56/150 Train Loss: 0.0083 Acc: 0.8131\n",
      "f1 score in 55th epoch is 0.6797840622326144\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7596\n",
      "Epoch: 57/150 Train Loss: 0.0083 Acc: 0.8162\n",
      "f1 score in 56th epoch is 0.6822487608421444\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7583\n",
      "Epoch: 58/150 Train Loss: 0.0083 Acc: 0.8149\n",
      "f1 score in 57th epoch is 0.6835823443014903\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7616\n",
      "Epoch: 59/150 Train Loss: 0.0082 Acc: 0.8159\n",
      "f1 score in 58th epoch is 0.6859077339323539\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7574\n",
      "Epoch: 60/150 Train Loss: 0.0082 Acc: 0.8171\n",
      "f1 score in 59th epoch is 0.6872520619856808\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7599\n",
      "Epoch: 61/150 Train Loss: 0.0082 Acc: 0.8181\n",
      "f1 score in 60th epoch is 0.6883266614447884\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7614\n",
      "Epoch: 62/150 Train Loss: 0.0082 Acc: 0.8174\n",
      "f1 score in 61th epoch is 0.6902407163294731\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7593\n",
      "Epoch: 63/150 Train Loss: 0.0082 Acc: 0.8172\n",
      "f1 score in 62th epoch is 0.6941694208602225\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7622\n",
      "Epoch: 64/150 Train Loss: 0.0081 Acc: 0.8206\n",
      "f1 score in 63th epoch is 0.69797317471496\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7637\n",
      "Epoch: 65/150 Train Loss: 0.0081 Acc: 0.8182\n",
      "f1 score in 64th epoch is 0.6979366450763641\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7606\n",
      "Epoch: 66/150 Train Loss: 0.0081 Acc: 0.8207\n",
      "f1 score in 65th epoch is 0.7007146930024618\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7609\n",
      "Epoch: 67/150 Train Loss: 0.0081 Acc: 0.8214\n",
      "f1 score in 66th epoch is 0.7004296497961283\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7613\n",
      "Epoch: 68/150 Train Loss: 0.0079 Acc: 0.8244\n",
      "f1 score in 67th epoch is 0.7031114684609925\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7596\n",
      "Epoch: 69/150 Train Loss: 0.0080 Acc: 0.8251\n",
      "f1 score in 68th epoch is 0.7038754436320539\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7584\n",
      "Epoch: 70/150 Train Loss: 0.0079 Acc: 0.8238\n",
      "f1 score in 69th epoch is 0.7054050338109635\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7607\n",
      "Epoch: 71/150 Train Loss: 0.0079 Acc: 0.8253\n",
      "f1 score in 70th epoch is 0.7060758759833543\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7605\n",
      "Epoch: 72/150 Train Loss: 0.0079 Acc: 0.8227\n",
      "f1 score in 71th epoch is 0.7075621695448131\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7613\n",
      "Epoch: 73/150 Train Loss: 0.0079 Acc: 0.8251\n",
      "f1 score in 72th epoch is 0.7089672277089691\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7613\n",
      "Epoch: 74/150 Train Loss: 0.0078 Acc: 0.8251\n",
      "f1 score in 73th epoch is 0.7094331379191241\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7613\n",
      "Epoch: 75/150 Train Loss: 0.0078 Acc: 0.8275\n",
      "f1 score in 74th epoch is 0.7123860835594621\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7632\n",
      "Epoch: 76/150 Train Loss: 0.0078 Acc: 0.8271\n",
      "f1 score in 75th epoch is 0.711482368430445\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7614\n",
      "Epoch: 77/150 Train Loss: 0.0077 Acc: 0.8299\n",
      "f1 score in 76th epoch is 0.7119211610514026\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7628\n",
      "Epoch: 78/150 Train Loss: 0.0077 Acc: 0.8291\n",
      "f1 score in 77th epoch is 0.7147371317604382\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7605\n",
      "Epoch: 79/150 Train Loss: 0.0077 Acc: 0.8292\n",
      "f1 score in 78th epoch is 0.7154039114771389\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7594\n",
      "Epoch: 80/150 Train Loss: 0.0077 Acc: 0.8295\n",
      "f1 score in 79th epoch is 0.715730053287862\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7633\n",
      "Epoch: 81/150 Train Loss: 0.0075 Acc: 0.8329\n",
      "f1 score in 80th epoch is 0.7192185466189481\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7627\n",
      "Epoch: 82/150 Train Loss: 0.0076 Acc: 0.8312\n",
      "f1 score in 81th epoch is 0.7203912098398473\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7591\n",
      "Epoch: 83/150 Train Loss: 0.0076 Acc: 0.8318\n",
      "f1 score in 82th epoch is 0.7215949907389521\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7622\n",
      "Epoch: 84/150 Train Loss: 0.0076 Acc: 0.8325\n",
      "f1 score in 83th epoch is 0.7234566047616278\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7616\n",
      "Epoch: 85/150 Train Loss: 0.0074 Acc: 0.8326\n",
      "f1 score in 84th epoch is 0.7246335473774574\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7623\n",
      "Epoch: 86/150 Train Loss: 0.0075 Acc: 0.8319\n",
      "f1 score in 85th epoch is 0.7263951634758841\n",
      "Begin test......\n",
      "Test Loss: 0.0109 Acc: 0.7627\n",
      "Epoch: 87/150 Train Loss: 0.0074 Acc: 0.8340\n",
      "f1 score in 86th epoch is 0.7281032192480457\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7617\n",
      "Epoch: 88/150 Train Loss: 0.0074 Acc: 0.8354\n",
      "f1 score in 87th epoch is 0.7278188211627619\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7639\n",
      "Epoch: 89/150 Train Loss: 0.0074 Acc: 0.8339\n",
      "f1 score in 88th epoch is 0.7294492006691503\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7638\n",
      "Epoch: 90/150 Train Loss: 0.0074 Acc: 0.8355\n",
      "f1 score in 89th epoch is 0.7305315198079867\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7623\n",
      "Epoch: 91/150 Train Loss: 0.0074 Acc: 0.8352\n",
      "f1 score in 90th epoch is 0.7320246851946693\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7639\n",
      "Epoch: 92/150 Train Loss: 0.0073 Acc: 0.8388\n",
      "f1 score in 91th epoch is 0.7335401271428149\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7617\n",
      "Epoch: 93/150 Train Loss: 0.0073 Acc: 0.8381\n",
      "f1 score in 92th epoch is 0.7343644245805182\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7651\n",
      "Epoch: 94/150 Train Loss: 0.0072 Acc: 0.8403\n",
      "f1 score in 93th epoch is 0.7359456189964508\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7619\n",
      "Epoch: 95/150 Train Loss: 0.0072 Acc: 0.8385\n",
      "f1 score in 94th epoch is 0.7373500651365761\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7605\n",
      "Epoch: 96/150 Train Loss: 0.0072 Acc: 0.8404\n",
      "f1 score in 95th epoch is 0.7382103865449801\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7625\n",
      "Epoch: 97/150 Train Loss: 0.0072 Acc: 0.8397\n",
      "f1 score in 96th epoch is 0.7402659652113737\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7646\n",
      "Epoch: 98/150 Train Loss: 0.0072 Acc: 0.8418\n",
      "f1 score in 97th epoch is 0.7417297646402924\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7593\n",
      "Epoch: 99/150 Train Loss: 0.0071 Acc: 0.8421\n",
      "f1 score in 98th epoch is 0.7437259258146267\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7617\n",
      "Epoch: 100/150 Train Loss: 0.0071 Acc: 0.8402\n",
      "f1 score in 99th epoch is 0.7462801404789381\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7637\n",
      "Epoch: 101/150 Train Loss: 0.0071 Acc: 0.8418\n",
      "f1 score in 100th epoch is 0.7476001715937193\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7632\n",
      "Epoch: 102/150 Train Loss: 0.0071 Acc: 0.8428\n",
      "f1 score in 101th epoch is 0.7487504811914082\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7638\n",
      "Epoch: 103/150 Train Loss: 0.0071 Acc: 0.8420\n",
      "f1 score in 102th epoch is 0.7493920468322517\n",
      "Begin test......\n",
      "Test Loss: 0.0110 Acc: 0.7646\n",
      "Epoch: 104/150 Train Loss: 0.0070 Acc: 0.8433\n",
      "f1 score in 103th epoch is 0.7505472246147323\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7628\n",
      "Epoch: 105/150 Train Loss: 0.0070 Acc: 0.8435\n",
      "f1 score in 104th epoch is 0.7529106064498022\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7626\n",
      "Epoch: 106/150 Train Loss: 0.0069 Acc: 0.8454\n",
      "f1 score in 105th epoch is 0.7552406139391544\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7628\n",
      "Epoch: 107/150 Train Loss: 0.0070 Acc: 0.8445\n",
      "f1 score in 106th epoch is 0.7557293799424993\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7638\n",
      "Epoch: 108/150 Train Loss: 0.0069 Acc: 0.8455\n",
      "f1 score in 107th epoch is 0.7574827859208635\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7607\n",
      "Epoch: 109/150 Train Loss: 0.0069 Acc: 0.8462\n",
      "f1 score in 108th epoch is 0.7591481380461799\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7613\n",
      "Epoch: 110/150 Train Loss: 0.0069 Acc: 0.8462\n",
      "f1 score in 109th epoch is 0.7596652014782487\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7630\n",
      "Epoch: 111/150 Train Loss: 0.0069 Acc: 0.8496\n",
      "f1 score in 110th epoch is 0.7606617182060477\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7620\n",
      "Epoch: 112/150 Train Loss: 0.0069 Acc: 0.8492\n",
      "f1 score in 111th epoch is 0.760001999345075\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7648\n",
      "Epoch: 113/150 Train Loss: 0.0068 Acc: 0.8487\n",
      "f1 score in 112th epoch is 0.7605032039986499\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7624\n",
      "Epoch: 114/150 Train Loss: 0.0068 Acc: 0.8483\n",
      "f1 score in 113th epoch is 0.7603325212406944\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7644\n",
      "Epoch: 115/150 Train Loss: 0.0068 Acc: 0.8498\n",
      "f1 score in 114th epoch is 0.7608054302309797\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7612\n",
      "Epoch: 116/150 Train Loss: 0.0067 Acc: 0.8501\n",
      "f1 score in 115th epoch is 0.7611691032633573\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7624\n",
      "Epoch: 117/150 Train Loss: 0.0068 Acc: 0.8500\n",
      "f1 score in 116th epoch is 0.7605312116786704\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7609\n",
      "Epoch: 118/150 Train Loss: 0.0067 Acc: 0.8523\n",
      "f1 score in 117th epoch is 0.7604433561817056\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7632\n",
      "Epoch: 119/150 Train Loss: 0.0067 Acc: 0.8502\n",
      "f1 score in 118th epoch is 0.760335364448969\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7638\n",
      "Epoch: 120/150 Train Loss: 0.0066 Acc: 0.8531\n",
      "f1 score in 119th epoch is 0.7613853255685696\n",
      "Begin test......\n",
      "Test Loss: 0.0111 Acc: 0.7623\n",
      "Epoch: 121/150 Train Loss: 0.0066 Acc: 0.8522\n",
      "f1 score in 120th epoch is 0.761729195113381\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7619\n",
      "Epoch: 122/150 Train Loss: 0.0066 Acc: 0.8537\n",
      "f1 score in 121th epoch is 0.7631776137772941\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7647\n",
      "Epoch: 123/150 Train Loss: 0.0065 Acc: 0.8554\n",
      "f1 score in 122th epoch is 0.763060350573685\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7627\n",
      "Epoch: 124/150 Train Loss: 0.0065 Acc: 0.8559\n",
      "f1 score in 123th epoch is 0.7624831261195332\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7598\n",
      "Epoch: 125/150 Train Loss: 0.0065 Acc: 0.8546\n",
      "f1 score in 124th epoch is 0.7634244958952364\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7620\n",
      "Epoch: 126/150 Train Loss: 0.0065 Acc: 0.8563\n",
      "f1 score in 125th epoch is 0.7643079284417984\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7641\n",
      "Epoch: 127/150 Train Loss: 0.0064 Acc: 0.8569\n",
      "f1 score in 126th epoch is 0.7652107534587391\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7622\n",
      "Epoch: 128/150 Train Loss: 0.0064 Acc: 0.8576\n",
      "f1 score in 127th epoch is 0.7650714469918691\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7615\n",
      "Epoch: 129/150 Train Loss: 0.0064 Acc: 0.8569\n",
      "f1 score in 128th epoch is 0.766880596378934\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7622\n",
      "Epoch: 130/150 Train Loss: 0.0064 Acc: 0.8565\n",
      "f1 score in 129th epoch is 0.7676779029990205\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7592\n",
      "Epoch: 131/150 Train Loss: 0.0064 Acc: 0.8577\n",
      "f1 score in 130th epoch is 0.767604209732634\n",
      "Begin test......\n",
      "Test Loss: 0.0112 Acc: 0.7626\n",
      "Epoch: 132/150 Train Loss: 0.0064 Acc: 0.8583\n",
      "f1 score in 131th epoch is 0.7683857873623743\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7633\n",
      "Epoch: 133/150 Train Loss: 0.0063 Acc: 0.8590\n",
      "f1 score in 132th epoch is 0.7687108941925777\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7627\n",
      "Epoch: 134/150 Train Loss: 0.0063 Acc: 0.8590\n",
      "f1 score in 133th epoch is 0.7686062039841204\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7607\n",
      "Epoch: 135/150 Train Loss: 0.0063 Acc: 0.8601\n",
      "f1 score in 134th epoch is 0.7688257712118505\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7621\n",
      "Epoch: 136/150 Train Loss: 0.0063 Acc: 0.8594\n",
      "f1 score in 135th epoch is 0.768671886333101\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7631\n",
      "Epoch: 137/150 Train Loss: 0.0063 Acc: 0.8608\n",
      "f1 score in 136th epoch is 0.769402524222633\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7609\n",
      "Epoch: 138/150 Train Loss: 0.0062 Acc: 0.8608\n",
      "f1 score in 137th epoch is 0.768408871183097\n",
      "Begin test......\n",
      "Test Loss: 0.0115 Acc: 0.7597\n",
      "Epoch: 139/150 Train Loss: 0.0062 Acc: 0.8611\n",
      "f1 score in 138th epoch is 0.7678342311659258\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7614\n",
      "Epoch: 140/150 Train Loss: 0.0062 Acc: 0.8602\n",
      "f1 score in 139th epoch is 0.7694972417743257\n",
      "Begin test......\n",
      "Test Loss: 0.0113 Acc: 0.7616\n",
      "Epoch: 141/150 Train Loss: 0.0062 Acc: 0.8620\n",
      "f1 score in 140th epoch is 0.7698069768617434\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7627\n",
      "Epoch: 142/150 Train Loss: 0.0061 Acc: 0.8651\n",
      "f1 score in 141th epoch is 0.7701467216799538\n",
      "Begin test......\n",
      "Test Loss: 0.0115 Acc: 0.7624\n",
      "Epoch: 143/150 Train Loss: 0.0061 Acc: 0.8643\n",
      "f1 score in 142th epoch is 0.7713454918950187\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7608\n",
      "Epoch: 144/150 Train Loss: 0.0061 Acc: 0.8633\n",
      "f1 score in 143th epoch is 0.7707043310063977\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7609\n",
      "Epoch: 145/150 Train Loss: 0.0061 Acc: 0.8636\n",
      "f1 score in 144th epoch is 0.770956653507549\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7625\n",
      "Epoch: 146/150 Train Loss: 0.0061 Acc: 0.8655\n",
      "f1 score in 145th epoch is 0.7716634832802389\n",
      "Begin test......\n",
      "Test Loss: 0.0115 Acc: 0.7603\n",
      "Epoch: 147/150 Train Loss: 0.0061 Acc: 0.8649\n",
      "f1 score in 146th epoch is 0.7716192650396193\n",
      "Begin test......\n",
      "Test Loss: 0.0114 Acc: 0.7635\n",
      "Epoch: 148/150 Train Loss: 0.0060 Acc: 0.8656\n",
      "f1 score in 147th epoch is 0.7731620337981163\n",
      "Begin test......\n",
      "Test Loss: 0.0115 Acc: 0.7616\n",
      "Epoch: 149/150 Train Loss: 0.0060 Acc: 0.8653\n",
      "f1 score in 148th epoch is 0.7730032799115858\n",
      "Begin test......\n",
      "Test Loss: 0.0115 Acc: 0.7611\n",
      "Epoch: 150/150 Train Loss: 0.0060 Acc: 0.8661\n",
      "f1 score in 149th epoch is 0.7741054055270663\n",
      "Begin test......\n",
      "Test Loss: 0.0116 Acc: 0.7605\n",
      "训练模型用时：813.4549822807312秒\n"
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "start_time = time.time()  # 记录开始时间\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "# 日志文件\n",
    "log_file = 'training_log.txt'\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        #######################\n",
    "        # 为使用L1loss function：\n",
    "       # target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "\n",
    "\n",
    "        # train model\n",
    "       # outputs, loss = train_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "        \n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        #########################\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    #########\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_targets.extend(target.cpu().numpy())\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    print(f\"f1 score in {epoch}th epoch is {f1}\")\n",
    "\n",
    "############\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "    # change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    \n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            ##################\n",
    "          #  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "          #  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "            #######################\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{NUM_EPOCHS}\\n')\n",
    "        f.write(f'Train Accuracy: {epoch_acc:.4f}%\\n')\n",
    "        f.write(f'Test Accuracy: {val_acc:.4f}%\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # save the model in last epoch\n",
    "        if (epoch +1) == NUM_EPOCHS:\n",
    "            \n",
    "            state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': (epoch+1),\n",
    "            }\n",
    "\n",
    "            # check the dir\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "\n",
    "            # save the state\n",
    "            torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch+1))))\n",
    "            \n",
    "end_time = time.time()  # 记录结束时间\n",
    "duration = end_time - start_time  # 计算训练时间\n",
    "print(f\"训练模型用时：{duration}秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Instance inference\n",
    "---\n",
    "The task is to visualizes an image along with model prediction and class probabilities.\n",
    "\n",
    "**To do**: \n",
    "1. Calculate the prediction and the probabilities for each class.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, classes = next(iter(test_dataloader))\n",
    "input = inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Write your answer here ##################\n",
    "# input: image, model\n",
    "# outputs: predict_label, probabilities\n",
    "# predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "###############################################################\n",
    "input_tensor = input.unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    probabilities = F.softmax(output, dim=1)\n",
    "    predict_label = torch.argmax(probabilities)\n",
    "# predict_label = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0qUlEQVR4nO3de1hU9do+8Hs8MELCIKIctoB4JEPJ2EpcnpVUOnnKLNs7T1uzwL3VLKNdHjqhud9Sewk7mGZFmZZappSioKZQchDQLVt5KfAV8NVyQFREWL8/+Dk5irIeYPwyeH+ua65LZh6e+a5ZwO2aWfOMQdM0DURERLdYM9ULICKi2xMDiIiIlGAAERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiVaqF7AtaqqqnDy5Ek4OzvDYDCoXg4REQlpmobS0lJ4e3ujWbMbH+c0ugA6efIkfHx8VC+DiIjqqaCgAB06dLjh7TYLoJiYGCxbtgxFRUUICgrCO++8g759+9b6fc7OzgCAtwE46rwvyUZIo03S+4Sw9xlBrXTdZkHtZWFv6Q+Nk6C2Qti7paDWQ9jbRVArfUxKhfXFgtrzwt4Skp9ZAPhFUCvZl4BsO6W9pb/LJYJaW/7+FAh7pwpqq4S9gT/+nt+ITQJo/fr1mDt3LlatWoWQkBAsX74cI0aMQE5ODtq3b3/T773ytJsj9AeQ5IfrDkGttLfkBwWQ/QJJe0v+kNvyjz7QeAKotbC3pF76mEgHMEoDy1ak4WYU1Eofw0ob9pbWS/6Q2nIt0j/otn6Ro7aXUWxyEsJbb72F6dOnY8qUKejRowdWrVoFJycnfPTRR7a4OyIiskMNHkCXLl1CamoqwsLC/riTZs0QFhaGAwcOXFdfXl6OkpISqwsRETV9DR5Ap0+fRmVlJTw8rJ9x9/DwQFFR0XX10dHRMJlMlgtPQCAiuj0ofx9QVFQUzGaz5VJQIH0ZjYiI7FGDn4Tg7u6O5s2bo7jY+ryd4uJieHp6XldvNBphNEpeqiQioqagwY+AHBwcEBwcjISEBMt1VVVVSEhIQGhoaEPfHRER2SmbnIY9d+5cTJo0CX/+85/Rt29fLF++HGVlZZgyZYot7o6IiOyQTQJowoQJ+L//+z8sWLAARUVFuPvuuxEfH3/diQlERHT7MmiaJn1PnE2VlJTAZDLhnwBa6fweyTv5uwvX01lQe1rYW/KmPncb9pa+MS5XWC95c6l0OyX1nYS9pY+LhPQNndmC2nxhb8kbH6S9JfW2fINmgLC3dP9I6iVTSgDZdkp/Nw8K66XMZjNcXG48U0T5WXBERHR7YgAREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESNpkF1xAqIRuxo5dkLAwgG4MhHd9hstE6pL2l6/YV1ktGFNlyBMoZYW/JY+5ow96A7OdWMrYHANYI6+3Rt8L6HsL6noJa6c+4k6BW8nsPAJLpnMW1l4jxCIiIiJRgABERkRIMICIiUoIBRERESjCAiIhICQYQEREpwQAiIiIlGEBERKQEA4iIiJRgABERkRIMICIiUqLRzoJrA/l8LT0kc5UAoJMN1lAXtngsbhUvQW3vZ4XNN+svXZ0ray1Zt/TnKl9YL5nvdjvMdrO1Izas9xf27i6olc4Y9BHUmgW1GoByHXU8AiIiIiUYQEREpAQDiIiIlGAAERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpESjHcUjIRk/IR1VcVpQKx2XUyColYzMAIASQW2FsLeUj2Smzb/0DPC4yj1G3aUFT8ha/ySolfycAMDXwnr75S6olT6K9ilPWC95VHoLe0v+TkjGTXEUDxERNWoMICIiUoIBRERESjCAiIhICQYQEREpwQAiIiIlGEBERKQEA4iIiJRgABERkRIMICIiUoIBRERESjTaWXC/ANA75WuboK9ZuI5HBLX3CXtLpmRdEPaWzLyTzoKTzIQCgPRC/bX3Pah/thsArP5Of+1bos5AqbD+9uArrJf8tJwX9pbW2yfJz6H0d9NTUCsJi0oAv+uo4xEQEREp0eABtGjRIhgMBqtLQEBAQ98NERHZOZs8BXfXXXdh586df9xJi0b7TB8RESlik2Ro0aIFPD0lzy4SEdHtxiavAR07dgze3t7o1KkTnnjiCeTn59+wtry8HCUlJVYXIiJq+ho8gEJCQrB27VrEx8cjNjYWeXl5GDBgAEpLaz6XIzo6GiaTyXLx8ZF+9icREdmjBg+g8PBwjB8/Hr169cKIESOwbds2nD17Fl9++WWN9VFRUTCbzZZLQYHkg6qJiMhe2fzsAFdXV3Tr1g3Hjx+v8Xaj0QijUfbeDyIisn82fx/QuXPnkJubCy8vL1vfFRER2ZEGD6B58+YhKSkJv/zyC/bv348xY8agefPmePzxxxv6roiIyI41+FNwJ06cwOOPP44zZ86gXbt26N+/P5KTk9GuXTtRn48AGHTWXhSvUr8YQa309AnJmB/pKB5HG9UCgPQ8RclYoEjBaB0A+FxQKx05dLvw79xZd+19/UNEvd//OE66HKqHHGF9oKD2tLC3Hg0eQF988UVDtyQioiaIs+CIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREgwgIiJSggFERERKMICIiEgJBhARESlh849jqKu20J+OJ2y5EIF0Yf1wQe15YW8JF2G9dN7UKkFtvLC3h6D2kQBZ70+PyurtVV5uru7aR56dJOqdJhgFd5DD+uotT1gvmV/pJKit1FnHIyAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREo02lE83337Lzjf4airtsvQCBuvRp/eLWX1+YLRI5IxGAAgWYpZ2Htwf1n96H3COxB4fWJn3bUVTvprAeDToz9Il9PkvTZvgaj+npCeumsP7suSLofqqUhQ21ZQa9BZxyMgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREgwgIiJSggFERERKMICIiEiJRjsLrtPAiXBxcdFV22O0/qFqRzbPFq3jvz74WnfttMKxot5bBGO1CkWdZfPd5mtrhN0ni6o1Qe2qO71FvR/pP1937aKVq0W96XqC8YUAAPN5/d/RTThL8T/SxdB1/iOo9RfUVums4xEQEREpwQAiIiIlGEBERKQEA4iIiJRgABERkRIMICIiUoIBRERESjCAiIhICQYQEREpwQAiIiIlGEBERKREo50FB9zx/y+1O7J5le6uY/91TLSKkAe66K417Q4Q9TbjqO5ad1Fn4GlB7XzhbDdbCnDvL6o3+d6ju7a7b6JsMUdTZPW3gZ3JB0T1n8dt1F3bb/hoUe+/L1kiqqf6ybNBTx4BERGREuIA2rNnDx566CF4e3vDYDBg8+bNVrdrmoYFCxbAy8sLjo6OCAsLw7FjsqMOIiJq+sQBVFZWhqCgIMTExNR4+5tvvomVK1di1apVSElJwR133IERI0bg4sWL9V4sERE1HeLXgMLDwxEeHl7jbZqmYfny5XjppZcwatQoAMC6devg4eGBzZs347HHHqvfaomIqMlo0NeA8vLyUFRUhLCwMMt1JpMJISEhOHCg5hcvy8vLUVJSYnUhIqKmr0EDqKioCADg4eFhdb2Hh4fltmtFR0fDZDJZLj4+Pg25JCIiaqSUnwUXFRUFs9lsuRQUFKheEhER3QINGkCenp4AgOLiYqvri4uLLbddy2g0wsXFxepCRERNX4MGkL+/Pzw9PZGQkGC5rqSkBCkpKQgNDW3IuyIiIjsnPgvu3LlzOH78uOXrvLw8ZGRkwM3NDb6+vpg9ezZee+01dO3aFf7+/nj55Zfh7e2N0aNHN+S6iYjIzokD6ODBgxgyZIjl67lz5wIAJk2ahLVr1+L5559HWVkZZsyYgbNnz6J///6Ij49Hq1atGm7V18nXXZmbWyjq7OmlfxQPQoaLeocIRvF0EnUGeghqDQaDqLemabLFmC/pLvXp7CTr7XRad+nML98QtX7HNU537RFRZ/vl1PNeUb3J60fdtT/t3iddDtk5cQANHjz4pn+ADAYDXnnlFbzyyiv1WhgRETVtys+CIyKi2xMDiIiIlGAAERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlBCP4mmczuuuzEnLEnX2wgDdteYUs6i3hLuw/iVB7URh7+wPd8m+IW2h7tIZH8vmge1/4X79xSY/Ue/Frzygu3b8gu9EvRuTF1+w3disQrP+3833N2+22TqoceIREBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJZrIKB79/vm3SaJ6J0Gt4YmPRb3/LKjd3lLUGgEVsnqJVSuniOrP5+brro15ZYhsMQGdZfUCjzy7THftQ3GyEULbjsrGNlWKqmXaenWyWe+CwtM26032j0dARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESt90suNFD7lC9BIuDgtqNwtluIbJykZgs/bPdAGCgoLb3y7tkixEozPpVVP/PVzfqrh3cf7yot5dXlqj+/d0ponqJ86KJhzL7UvRvp4dXgKh3ceFR6XKokeEREBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJZrEKB5/37/rrm1rw3V4DP+XqL74h3m6a7cI11Ihma5yXtbbQ1YOrwDbjXqR+GfUO6L6tKO5umt35G4W9T4hqratnLgZumuzO5tFve8frn8o1MtLloh60631Z0FtJYB0HXU8AiIiIiUYQEREpIQ4gPbs2YOHHnoI3t7eMBgM2Lx5s9XtkydPhsFgsLqMHDmyodZLRERNhDiAysrKEBQUhJiYmBvWjBw5EoWFhZbL559/Xq9FEhFR0yM+CSE8PBzh4eE3rTEajfD09KzzooiIqOmzyWtAiYmJaN++Pbp3746nn34aZ86cuWFteXk5SkpKrC5ERNT0NXgAjRw5EuvWrUNCQgKWLl2KpKQkhIeHo7Kyssb66OhomEwmy8XHx6ehl0RERI1Qg78P6LHHHrP8u2fPnujVqxc6d+6MxMREDBs27Lr6qKgozJ071/J1SUkJQ4iI6DZg89OwO3XqBHd3dxw/frzG241GI1xcXKwuRETU9Nk8gE6cOIEzZ87Ay8vL1ndFRER2RPwU3Llz56yOZvLy8pCRkQE3Nze4ublh8eLFGDduHDw9PZGbm4vnn38eXbp0wYgRIxp04UREZN/EAXTw4EEMGTLE8vWV128mTZqE2NhYZGZm4uOPP8bZs2fh7e2N4cOH49VXX4XRaGy4VV/jvvHzddeekY2yguS4bednz4p692ynfxbcDqf+ot7nW+7TXTtQ1BnYI6zv23+87trTKami3j+l7NBdW5j/oaj3oVzhD4uder3nad2122KniHqfMen/uQ3q7C7qfShX/7qpZgsFtfonIwKXoG8WnDiABg8eDE3Tbnj7999/L21JRES3Ic6CIyIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkhEG72VwdBUpKSmAymWA2m3V/NMOOo/r7t2wpW8/gzvprK2St4WAwCL9Dv9hX3tddm7Nvhqj38h9ka3ET1P4ma001+Itw8PynhbZZBwA4C2q9TLJfzv+Ypb9xTd8EYb3gzxvSBLWXAewEav07ziMgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREgwgIiJSggFERERKtFC9gIZwX4D+2nRh79OCWpOwNwIm6a89+rGo9ba087pr+4VEi3oHZUWJ6g/ZcNRLD0HtEWFvyQihx/8m2JcAYj6U7U8J3xBfUb3/5nzdtXnCtZRKam+T0TrdhPWRglrhFCbo3/NAiKC2HNWjeGrDIyAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAmDpmma6kVcraSkBCaTCWazGS4uLg3eXzL7CJDNd5POgvvrq7t01366YJiw+0T9pS31z40DgND+solTnc9n6a79NGWfqLdkXttvos7AlJCeums/Ss4U9TYY2glXo38qYQdhZ5+W+mvNXu6i3kfyJdMUbw8/COu7C2r/R9hb8vcwW1BbDmAlUOvfcR4BERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREgwgIiJSooXqBdxI8pef4Q4nR121/SZO1t3XV7qQCkGtYKQJAHQPCJB9g0CH/vfprv1x72RRbz+DQVR/QFQtIxuv4ySq/mivbLyOxIsvx4jq33h1gu7aE8K1nJD8jHO0Tr2tk/0YAoJJWVuErSW7/qKwtx48AiIiIiVEARQdHY0+ffrA2dkZ7du3x+jRo5GTk2NVc/HiRURERKBt27Zo3bo1xo0bh+Li4gZdNBER2T9RACUlJSEiIgLJycnYsWMHKioqMHz4cJSVlVlq5syZg2+//RYbNmxAUlISTp48ibFjxzb4womIyL6JXgOKj4+3+nrt2rVo3749UlNTMXDgQJjNZqxevRpxcXEYOnQoAGDNmjW48847kZycjHvvvbfhVk5ERHatXq8Bmc1mAICbW/WnsqSmpqKiogJhYWGWmoCAAPj6+uLAgZpfii4vL0dJSYnVhYiImr46B1BVVRVmz56Nfv36ITAwEABQVFQEBwcHuLq6WtV6eHigqKioxj7R0dEwmUyWi4+PT12XREREdqTOARQREYHs7Gx88cUX9VpAVFQUzGaz5VJQUFCvfkREZB/q9D6gyMhIbN26FXv27EGHDn98ALCnpycuXbqEs2fPWh0FFRcXw9PTs8ZeRqMRRqOxLssgIiI7JjoC0jQNkZGR2LRpE3bt2gV/f3+r24ODg9GyZUskJCRYrsvJyUF+fj5CQ0MbZsVERNQkiI6AIiIiEBcXhy1btsDZ2dnyuo7JZIKjoyNMJhOmTZuGuXPnws3NDS4uLpg1axZCQ0N5BhwREVkRBVBsbCwAYPDgwVbXr1mzBpMnTwYAvP3222jWrBnGjRuH8vJyjBgxAu+++26DLJaIiJoOg6ZpmupFXK2kpAQmkwlAEIDmur5H01Jtt6BCQa2XrPWgZz7TXbsn9i+i3i+++4vu2tef9hP1lur+qP7/gLRoaRL1fuCB8bprX5/oIOotHO0nki2Y7wUAPQes0F+cNlvWnBqAZK6jWdhb8pOYL+wtIVmHBuAyzGYzXFxcbljFWXBERKQEA4iIiJRgABERkRIMICIiUoIBRERESjCAiIhICQYQEREpwQAiIiIlGEBERKQEA4iIiJSo08cx3Bo9AOgbnSKZalIhXIWTYDKMdHSLk6S5kK3H60jkfPmM7lrD9H+Leh9Z8p3u2mXfDRH1xlHJWJPTst5ZwvqKpbJ6uyT9DbpHUOsr7C39SyGZwyVdi+RxSRP2lqxbMpesAsCGWqt4BEREREowgIiISAkGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAkGEBERKdGIZ8E5Qe8suDtG6O/6lw9kq5gvGNsUKGuNl154UH/t38qF3RsPw9C9+ot3D7TdQrJs11pOOvdMMrNLyklQ29Nmq6jDNEUb9g4Q1ksmUgrnAIp6Sx4Tab3kMdR0VfEIiIiIlGAAERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREo14FE8HAK30lf4wRnfXT/3cRatIfFf/7J6vnha1xl8fnae79n92/UvW3IYe/k74DbsXCopl+0c2HkQ6pqRCUCsdryIdDWO2YW/JqJdCYW/JCCHJ4w3I9qd03/sI60uE9RJtBbWSfQkAZwS1o4Tr+LzWKh4BERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREgwgIiJSggFERERKNOJZcFMBuOislcyQ2ixaxYln/qG7Nqxwhai3fN5U4/Dtg+2E33GPoLazsLd0xpdEvqBWOsdMOrPLlkw2qgUAXxv2lswN7CfsLf25kszIk/aWzNOT/lwN1V86XND2cgmwq/YyHgEREZESogCKjo5Gnz594OzsjPbt22P06NHIycmxqhk8eDAMBoPVZebMmQ26aCIisn+iAEpKSkJERASSk5OxY8cOVFRUYPjw4SgrK7Oqmz59OgoLCy2XN998s0EXTURE9k/0GlB8fLzV12vXrkX79u2RmpqKgQMHWq53cnKCp6dnw6yQiIiapHq9BmQ2V39Ilpubm9X1n332Gdzd3REYGIioqCicP3/jF8bKy8tRUlJidSEioqavzmfBVVVVYfbs2ejXrx8CAwMt10+cOBF+fn7w9vZGZmYm5s+fj5ycHHz99dc19omOjsbixYvrugwiIrJTdQ6giIgIZGdnY9++fVbXz5gxw/Lvnj17wsvLC8OGDUNubi46d77+FNuoqCjMnTvX8nVJSQl8fOzz9GQiItKvTgEUGRmJrVu3Ys+ePejQocNNa0NCQgAAx48frzGAjEYjjEZjXZZBRER2TBRAmqZh1qxZ2LRpExITE+Hv71/r92RkZAAAvLwkb6YiIqKmThRAERERiIuLw5YtW+Ds7IyioiIAgMlkgqOjI3JzcxEXF4f7778fbdu2RWZmJubMmYOBAweiV69eNtkAIiKyT6IAio2NBVD9ZtOrrVmzBpMnT4aDgwN27tyJ5cuXo6ysDD4+Phg3bhxeeumlBlswERE1DeKn4G7Gx8cHSUlJ9VrQH0zQPwuuk6CvZDYVAPygu7L01QGizs5/TxCuxTYM0/cKv+O0sF4yJytA2Fsy+ypN2FtCMpfM1qRrkdRLe0vmAAbWXmJFshbpz+xPwnrJLEDpvLZp+ks7d5G1lvy67RbU3jwqLDgLjoiIlGAAERGREgwgIiJSggFERERKMICIiEgJBhARESnBACIiIiUYQEREpAQDiIiIlGAAERGREnX+PCDbM0P3PAc4CfpKR72YBLW5os6lKxfqL14RLeotsiFR+A39hfW2ewyBnoJa6RgZyVok21iXeluOy7Hl749ZUCsdfyMZ3SMZlQPIx+XkC2oFo3UAwFcwXkc6cei7/xIUS/Zlua4qHgEREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREo04llwl6F/fpNkbpOncB2SmV1ewt67dVcagn8VdT6e6qe/2Bwr6i2bHSat9xX2ltRL1y2Ze5Yi7C0d2iX5GZfOgpP8jEvXLektndf2ow17S2f1Pa6/1GmArHX+JUHxUllvLBDU/l1Qy1lwRETUiDGAiIhICQYQEREp0YhfAyIia5cBVOmoOyvsK/kzIP2TIXnt6rKw90Ub9r4grD+mv7SqXQ1XugPNpK9/2j8GEJFduAzgpM7aIlsuhGq0Vn9pjbnZCmiVc9uFEJ+CI7ILeo58yH5dhPwMQ/vHACIiIiUYQEREpAQDiIiIlGAAEdmxvLw8HD16FOnp6Th8+DCeeeaZeve86667kJeXBwDw8vLCnj17av2ef/zjH/Dw8KjT/S1btgwLFy6s8bbmzZtjwYIF+Pe//42srCykp6fjvffeg8lkwqBBg5Cenl6n+7yVRo0ahZCQENXLaJQa71lwLdwBg4u+2oqWgsYFwoUECmolp5wCQGf9pWkdRZ27GCYJqgtFveVyBbXSESiSx1zweAOQjfkxC3tLt9MI4NMab5kwYQIOHToEX19fZGZmYu/evcjKyrLcbjAYAACapgnvEygsLMTAgQNrrZs9ezYSExNRXFwsvo+bWb16Ndzc3BAaGoqzZ88CAB555BG4ubk16P3Y0ujRo5GRkYGUlFrGNXkCaFXD9UcdBPfmI6iVyqq9xELfae88AiJqIvLz85GTk4Nu3bph4cKF2LhxI+Lj45GdnQ0vLy8MHz4ce/fuxcGDB5GSkoLBgwdbvnfhwoX4z3/+g4MHD+Kxxx6zXO/n54fff//d8vW9996LvXv3IiMjA4cOHcLDDz+Ml19+Gd7e3li/fj3S09MRFBSEFi1aIDo6GikpKUhPT8f69evh6uoKAPD09ER8fDwOHz6MHTt2oEOHDjVuT+fOnTF+/HhMmTLFEj4AsHHjRssR2hXNmzdHfHw8fv75Z2RnZ+Ozzz6Dk1P17L8uXbpg3759yMjIQGZmJl599VUAwIMPPohDhw4hPT0dWVlZePjhh2t9jF1cXPDBBx8gKysLGRkZWL16NQBg6NCh2L9/P9LS0pCdnY2pU6cCAMLDw/Hwww/jueeeQ3p6OqZNm1brfdxOGu8REBGJBAYGIiAgAIcOHUJgYCBCQ0PRu3dvnDp1Cv7+/li0aBFGjBiB0tJSdO7cGXv37kXHjh0RFhaG8ePHIzg4GKWlpfjkk09q7N+mTRts3rwZjzzyCPbt2weDwQBXV1d88803mDp1quVIDACioqJQVlZmeerppZdewmuvvYbIyEisXLkSP/30E0aOHAlvb29kZGTg6NGj193fPffcg2PHjuHMmTO1bntlZSUmTpyI3377DQDw7rvvYtasWVi6dCkiIyOxdetWLFmyxLIdAPDaa6/hqaeeQnJyMgwGA1xcqp9xeeqpp+Dt7V3j04LLly/HhQsX0KtXL2iaBnf36sGvaWlp6N+/P6qqqtCmTRukp6fj+++/x/bt2/HNN98gIyMDK1asqHU7bjcMICI7t379ely4cAHnz5/H1KlTcfz4cQDAtm3bcOrUKQDAyJEj0aVLF6vXc6qqquDr64thw4bhyy+/RGlpKQDgvffeQ//+/a+7n9DQUOTk5GDfvn0Aqp/Su/ro6GqjR4+GyWTCuHHjAAAODg745ZdfAADDhg3DvHnzAAAnT57EN998U+/HwGAwYM6cOXjggQfQokULmEwm7N+/HwCwZ88eLFu2DK1bt0ZSUhJ27twJAEhISMCKFSuwceNG/PDDD5bwfO+99254Pw8++CBCQkIsT2eePl393p22bdti9erV6NatGy5fvoy2bdsiMDAQ//u//1vvbWvKGEBEdu7qI4+rnTt3zvJvg8GAHTt24Iknnqi1X11eK7qWwWDArFmzsGPHjjrfX1paGrp27Qo3NzfLkc2NTJw4EUOHDsWgQYNQWlqKWbNmYejQoQCAr7/+Gvv378d9992HyMhIzJ49Gw888ACeffZZ9OjRA0OGDMHHH3+Mzz77DMuWLZNvLIBVq1Zh27ZtlsBNTU1Fq1Y1vaBDV+NrQES3ge+//x5hYWHo2bOn5bo+ffoAAHbu3Inx48ejdevWAIAZM2bU2GP//v3o2rWr5ejIYDBYns4qKSmByfTHiRWbN2/GnDlz4OjoCABwdHREjx49LPd35TUST0/PG772kpubi6+++gqrV6+26j127Fj4+/tb1bZp0wanT59GaWkpWrdujcmTJ1tu69KlC4qLi/HJJ5/g+eefx7333gsA6N69O44cOYKYmBjExsZarr+Zb775BvPmzbOc2HHlKbg2bdrg11+rP7NrwIABCAoKsnzPtY8N/YEBRHQbyM3NxcSJE/Hee+8hIyMDR44cwezZswEA27dvx8aNG5GWloaDBw8iPz+/xh5nz57FmDFjsGTJEhw6dAhpaWno168fAGDlypX44IMPLCchLF26FD///DNSUlJw6NAhJCcn4+677wZQfcr2vffei8OHD2PdunXYtWvXDdc9depUHDp0CCkpKcjOzsaRI0cwfPjw646I1q1bBycnJxw9ehTbt2/H3r17Lbc98sgjyMrKQlpaGtavX4+ZM2cCAN544w1kZ2cjLS0Nf/3rX7Fo0SIA1a8BLV68uMb1zJkzB0aj0XJK+BtvvAEAeOGFF7BkyRKkp6dj6tSpVme8ffLJJ3j00UeRlpbGkxCuYdAa4ni7AVn+t9DCLDgN+0vBPfwkXJHkNOz/EfaWnJ4cJ+wtOQ37Y2FvW5L+T1Hy/grpadiST1CVnKIKNORp2NREdEwFWt1z/fXXn59xE2uFdzpFUDtEUHsZwF6YzWbLyR014REQEREpwQAiIiIlGEBEdqwhRvFMmjQJmzZtEn/fwoUL8fbbb9d421NPPWU51frq/sHBwfjiiy8AACaTCfPnzxff77UcHR0RFxeHY8eOIScnx3ImWk3+8pe/ICMjA1lZWdi5cyd8fKonB7i5uSE9Pd1yycnJQUVFheUkC7INnoZNZOdsOYqnrm70XprU1FTLpAVXV1e88MILWLp0ab3ua968eSgvL0fXrl3RsWNHpKSkYPfu3dedqNC9e3csW7YMvXv3RlFREZ544gnExsbiwQcfxG+//YbevXtbap999lkMGjTohu9zoobReANI+gm6uknnntlytpLkRW7pMEPJiQWSWXoAUCGsl5DOVPvBJquoJnlcvIS9A4T1tZ+Qc/UonrFjx6Jnz55o3bo1fHx8cN9992Ho0KF47rnnAAAFBQWYMWMGTp6s/pRVFxcXbNmyBV26dMHp06fx5JNP4tdff0VgYCBiY2Ph5OSEVq1aIS4uDq+//rrlPn18fJCQkABvb28cO3YMkydPxm+//YaFCxfC1dUVc+bMsVrjoEGDsHz5cvTu3RurVq2Cs7Mz0tPTcfnyZcycOROffvop7rzzTkv9jz/+iFdffRXx8fE33O4JEyZYzi775ZdfkJiYiDFjxljG5FwRGBiIzMxMFBVVf2Lstm3bsG7duhrfZzRt2jRERUXV+pg3qHsBtK/h+qOXBE22CO/0+jcc39huYe/a8Sk4oibi6lE8QPXkgieffBJ33XUX2rRpg2XLliE8PBxBQUHYv38/PvzwQ8v39uvXD/Pnz8ddd92FrVu34v333wdQ/Qd92LBhCA4ORnBwMMaNG2c12XnAgAGYOHEi7rzzThQUFCA6Olr3emfOnInS0lL07t0bffr0QWpqKs6cOYP77rsPAHD33XejXbt2iI+Px+LFi/HUU0/V2MfX19fyHpwra/b1vX6Q7KFDh3DPPfega9euAKqfjmvWrBn8/Pys6kJDQ9GmTRts3bpV97ZQ3YgCKDY2Fr169YKLiwtcXFwQGhqK7du3W26/ePEiIiIi0LZtW7Ru3Rrjxo1r8Om4RGTtyhDQ995774ajeIYMGYL4+HjLEc+7776LoUOHolmz6j8B+/fvt8xje//99zF48GA0a9YMjo6O+PDDD5GZmYnk5GT4+flZ3s8DAN99953ld/z9999HWFhYvbZlxYoViIyMBABERETg3XffBVD9etPNRuTocfz4ccycORPr1q3Dzz//jLZt2+L333/H5cvWT7dMmzYN69atQ2VlZb3uj2onCqAOHTpgyZIlSE1NxcGDBzF06FCMGjUKhw8fBlD9Jq1vv/0WGzZsQFJSEk6ePImxY8faZOFEVG3ChAno3bs3+vXrh6+++spy/dWjeK6l9/WgN954A6dPn0bv3r1x9913IzEx8aYjZur7OtPXX3+NXr164e6778bDDz+MNWvW1Po9+fn5VkcxHTt2vOGbab/66iuEhoaiT58+iI2NhaOjoyWwAeCOO+7Ao48+io8++qhe20H6iALooYcewv3334+uXbuiW7dueP3119G6dWskJyfDbDZj9erVeOuttzB06FAEBwdjzZo12L9/P5KTk221fiLSYffu3Rg5ciS8vKpfq5o5cyYSEhJQVVUFoPppp+7duwMA/va3v2H37t2Wyc4nTpxAZWUlunXrZnl67Ir7778f7du3t3zflUGfepSUlMDR0REtW/7xWltlZSVWrVqFb775Bps2bYLZXPtrghs2bLBMN+jYsSMGDx6MzZs311jr6ekJAGjWrBmWLl2KmJgYXLhwwXL7lRM6cnJydG8H1V2dT0KorKzEhg0bUFZWhtDQUKSmpqKiosLqEDwgIAC+vr44cODADecslZeXo7y83PJ1SUlJXZdERDdw+PBhPPfcc5YX8wsKCjB9+nTL7fv378fSpUvRpUsXnDlzBk8++SSA6o8s+OSTTzBp0iTk5uZeNzZn7969iIuLw5/+9CfLSQh6/f7771i3bh0yMzNx7tw5y2y61atX44033sB///d/W2oXL16MkydP1vg03LJly/DRRx/h+PHjqKysRGRkpOUjHK79aIWPPvoIfn5+MBqN+O677/Diiy9a9Zo2bRo++OAD3dtA9SMexZOVlYXQ0FBcvHgRrVu3RlxcHO6//37ExcVhypQpVmECAH379sWQIUNueKrlokWLbjB3yQw9Z/5Uk4zikZ4l0ldQW/vnlliTnJEnHfVSy6cvWmlMZ8E1Jo3tLLiNwu+xT+PGjcPTTz9d79eT7M5jqUD7GkbxrJScBTdBeKenBbX7hL1R6yge8RFQ9+7dkZGRAbPZjI0bN2LSpElISkoSL+yKqKgozJ071/J1SUmJ5c1hRHR72b59O7p164YxY8aoXgrdAuIAcnBwQJcuXQBUv6v5559/xooVKzBhwgRcunQJZ8+etXz0LgAUFxdbnnetidFohNFolK+ciJqc8PBw1UugW6je7wOqqqpCeXk5goOD0bJlSyQkJFhuy8nJQX5+PkJDQ+t7N0RE1MSIjoCioqIQHh4OX19flJaWIi4uDomJifj+++9hMpkwbdo0zJ07F25ubnBxccGsWbMQGhqq64OeiIjo9iIKoFOnTuHJJ59EYWEhTCYTevXqhe+//95yaubbb7+NZs2aYdy4cSgvL8eIESMsbySTKwRw4/cxWJO+iC6RbcPe521UC8gek9vlpAIpyeNS8/tOGq7eVVhPducogBM13SAZqyX9uZKePNOwRAF07Wyla7Vq1QoxMTGIiYmp16KI6FoG1QsgWzK0Alq4q17FLdd4h5ES0VWaA2gDQM+7Jh4U9u4nqK3xv+g3IXk7gHQCsWQtwtOTOwonuEhmetb0pEoLd6DV9fPrmjoGEJHdaK6zzkPYt5sN1nDFfwS10qeCJe+786u95Go1fTT2zUge8l9krZsyTsMmIiIlGEBERKQEA4iIiJRgABERkRIMICIiUoIBRERESjCAiIhIiUb3PqA/Pp5I7xgeQDamRvpeA8lncUhJ1iL9fPr6fTQyNTZVgtry2kuslAlqL9ReYkXyMy793ZT8TgjXXSn8YEzJQy59v61o7dK/E7Ydw1Xbx82JP5DO1k6cOMHPAyIiagIKCgrQoUOHG97e6AKoqqoKJ0+ehLOzMwyGP+ZfXfmguoKCgpt+wp6943Y2HbfDNgLczqamIbZT0zSUlpbC29sbzZrd+JWeRvcUXLNmzW6amC4uLk1651/B7Ww6bodtBLidTU19t9NkMtVaw5MQiIhICQYQEREpYTcBZDQasXDhQhiNRtVLsSluZ9NxO2wjwO1sam7ldja6kxCIiOj2YDdHQERE1LQwgIiISAkGEBERKcEAIiIiJewmgGJiYtCxY0e0atUKISEh+Omnn1QvqUEtWrQIBoPB6hIQEKB6WfWyZ88ePPTQQ/D29obBYMDmzZutbtc0DQsWLICXlxccHR0RFhaGY8eOqVlsPdS2nZMnT75u344cOVLNYusoOjoaffr0gbOzM9q3b4/Ro0cjJyfHqubixYuIiIhA27Zt0bp1a4wbNw7FxcWKVlw3erZz8ODB1+3PmTNnKlpx3cTGxqJXr16WN5uGhoZi+/btlttv1b60iwBav3495s6di4ULFyItLQ1BQUEYMWIETp06pXppDequu+5CYWGh5bJv3z7VS6qXsrIyBAUFISYmpsbb33zzTaxcuRKrVq1CSkoK7rjjDowYMQIXL168xSutn9q2EwBGjhxptW8///zzW7jC+ktKSkJERASSk5OxY8cOVFRUYPjw4Sgr+2OQ6Zw5c/Dtt99iw4YNSEpKwsmTJzF27FiFq5bTs50AMH36dKv9+eabbypacd106NABS5YsQWpqKg4ePIihQ4di1KhROHz4MIBbuC81O9C3b18tIiLC8nVlZaXm7e2tRUdHK1xVw1q4cKEWFBSkehk2A0DbtGmT5euqqirN09NTW7ZsmeW6s2fPakajUfv8888VrLBhXLudmqZpkyZN0kaNGqVkPbZy6tQpDYCWlJSkaVr1vmvZsqW2YcMGS82///1vDYB24MABVcust2u3U9M0bdCgQdo//vEPdYuykTZt2mgffvjhLd2Xjf4I6NKlS0hNTUVYWJjlumbNmiEsLAwHDhxQuLKGd+zYMXh7e6NTp0544oknkJ+fr3pJNpOXl4eioiKr/WoymRASEtLk9isAJCYmon379ujevTuefvppnDlzRvWS6sVsNgMA3NzcAACpqamoqKiw2p8BAQHw9fW16/157XZe8dlnn8Hd3R2BgYGIiorC+fOSj4RpXCorK/HFF1+grKwMoaGht3RfNrphpNc6ffo0Kisr4eHhYXW9h4cHjh49qmhVDS8kJARr165F9+7dUVhYiMWLF2PAgAHIzs6Gs7Oz6uU1uKKiIgCocb9eua2pGDlyJMaOHQt/f3/k5ubixRdfRHh4OA4cOIDmzZurXp5YVVUVZs+ejX79+iEwMBBA9f50cHCAq6urVa0978+athMAJk6cCD8/P3h7eyMzMxPz589HTk4Ovv76a4WrlcvKykJoaCguXryI1q1bY9OmTejRowcyMjJu2b5s9AF0uwgPD7f8u1evXggJCYGfnx++/PJLTJs2TeHKqL4ee+wxy7979uyJXr16oXPnzkhMTMSwYcMUrqxuIiIikJ2dbfevUdbmRts5Y8YMy7979uwJLy8vDBs2DLm5uejcufOtXmadde/eHRkZGTCbzdi4cSMmTZqEpKSkW7qGRv8UnLu7O5o3b37dGRjFxcXw9PRUtCrbc3V1Rbdu3XD8+HHVS7GJK/vudtuvANCpUye4u7vb5b6NjIzE1q1bsXv3bquPTfH09MSlS5dw9uxZq3p73Z832s6ahISEAIDd7U8HBwd06dIFwcHBiI6ORlBQEFasWHFL92WjDyAHBwcEBwcjISHBcl1VVRUSEhIQGhqqcGW2de7cOeTm5sLLy0v1UmzC398fnp6eVvu1pKQEKSkpTXq/AtWf+nvmzBm72reapiEyMhKbNm3Crl274O/vb3V7cHAwWrZsabU/c3JykJ+fb1f7s7btrElGRgYA2NX+rElVVRXKy8tv7b5s0FMabOSLL77QjEajtnbtWu3IkSPajBkzNFdXV62oqEj10hrMs88+qyUmJmp5eXnajz/+qIWFhWnu7u7aqVOnVC+tzkpLS7X09HQtPT1dA6C99dZbWnp6uvbrr79qmqZpS5Ys0VxdXbUtW7ZomZmZ2qhRozR/f3/twoULilcuc7PtLC0t1ebNm6cdOHBAy8vL03bu3Kndc889WteuXbWLFy+qXrpuTz/9tGYymbTExEStsLDQcjl//rylZubMmZqvr6+2a9cu7eDBg1poaKgWGhqqcNVytW3n8ePHtVdeeUU7ePCglpeXp23ZskXr1KmTNnDgQMUrl3nhhRe0pKQkLS8vT8vMzNReeOEFzWAwaD/88IOmabduX9pFAGmapr3zzjuar6+v5uDgoPXt21dLTk5WvaQGNWHCBM3Ly0tzcHDQ/vSnP2kTJkzQjh8/rnpZ9bJ7924NwHWXSZMmaZpWfSr2yy+/rHl4eGhGo1EbNmyYlpOTo3bRdXCz7Tx//rw2fPhwrV27dlrLli01Pz8/bfr06Xb3n6eatg+AtmbNGkvNhQsXtGeeeUZr06aN5uTkpI0ZM0YrLCxUt+g6qG078/PztYEDB2pubm6a0WjUunTpoj333HOa2WxWu3ChqVOnan5+fpqDg4PWrl07bdiwYZbw0bRbty/5cQxERKREo38NiIiImiYGEBERKcEAIiIiJRhARESkBAOIiIiUYAAREZESDCAiIlKCAUREREowgIiISAkGEBERKcEAIiIiJRhARESkxP8DxnYiiQn54mAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print probabilities for each class:\n",
      "airplane: 0.0001\n",
      "automobile: 0.0000\n",
      "bird: 0.0002\n",
      "cat: 0.9748\n",
      "deer: 0.0001\n",
      "dog: 0.0239\n",
      "frog: 0.0003\n",
      "horse: 0.0002\n",
      "ship: 0.0003\n",
      "truck: 0.0001\n"
     ]
    }
   ],
   "source": [
    "predicted_class = class_names[predict_label.item()]\n",
    "predicted_probability = probabilities[0][predict_label.item()]\n",
    "image = input.numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image)\n",
    "plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}', \n",
    "            color='white', backgroundcolor='black', fontsize=8)\n",
    "plt.show()\n",
    "\n",
    "# Print probabilities for each class\n",
    "print('Print probabilities for each class:')\n",
    "for i in range(len(class_names)):\n",
    "    print(f'{class_names[i]}: {probabilities[0][i].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20844,0.30368,0.36178,0.40706,0.44480,0.47278,0.50292,0.52614,0.54916,0.56924,0.59030,0.60876,0.62570,0.63890,0.65120,0.66482,0.67402,0.68378,0.69200,0.70184,0.70610,0.71620,0.72168,0.72972,0.73322,0.74014,0.74544,0.74904,0.75370,0.75956,0.78750,0.79212,0.79152,0.79540,0.79650,0.79888,0.79988,0.79926,0.79898,0.80212,0.80360,0.80282,0.80586,0.80420,0.80670,0.80866,0.80516,0.80856,0.80796,0.80864,0.81048,0.81036,0.81246,0.81394,0.81162,0.81308,0.81620,0.81490,0.81592,0.81714,0.81806,0.81744,0.81716,0.82064,0.81816,0.82074,0.82138,0.82442,0.82506,0.82382,0.82532,0.82270,0.82508,0.82506,0.82754,0.82714,0.82988,0.82906,0.82916,0.82950,0.83294,0.83120,0.83178,0.83250,0.83256,0.83186,0.83396,0.83540,0.83388,0.83548,0.83518,0.83880,0.83814,0.84032,0.83846,0.84042,0.83968,0.84176,0.84208,0.84020,0.84180,0.84284,0.84198,0.84330,0.84350,0.84544,0.84452,0.84554,0.84618,0.84616,0.84962,0.84920,0.84874,0.84828,0.84982,0.85008,0.84998,0.85228,0.85024,0.85308,0.85220,0.85372,0.85544,0.85594,0.85460,0.85632,0.85692,0.85756,0.85694,0.85654,0.85774,0.85828,0.85896,0.85896,0.86012,0.85942,0.86078,0.86080,0.86112,0.86022,0.86196,0.86508,0.86426,0.86328,0.86360,0.86546,0.86488,0.86558,0.86526,0.86608,\n",
      "\n",
      "\n",
      "0.34420,0.40770,0.45090,0.48210,0.50880,0.53040,0.55090,0.56860,0.59370,0.61540,0.62470,0.63570,0.65530,0.66690,0.66720,0.67300,0.68530,0.68770,0.70520,0.70420,0.69800,0.71260,0.71890,0.72460,0.72250,0.72900,0.72920,0.73150,0.74270,0.73680,0.75710,0.75880,0.75870,0.75790,0.76180,0.75880,0.75810,0.75630,0.75920,0.75640,0.75620,0.75860,0.75760,0.75810,0.76030,0.75810,0.75970,0.76110,0.76090,0.76160,0.75970,0.76100,0.75880,0.75820,0.75870,0.75960,0.75830,0.76160,0.75740,0.75990,0.76140,0.75930,0.76220,0.76370,0.76060,0.76090,0.76130,0.75960,0.75840,0.76070,0.76050,0.76130,0.76130,0.76130,0.76320,0.76140,0.76280,0.76050,0.75940,0.76330,0.76270,0.75910,0.76220,0.76160,0.76230,0.76270,0.76170,0.76390,0.76380,0.76230,0.76390,0.76170,0.76510,0.76190,0.76050,0.76250,0.76460,0.75930,0.76170,0.76370,0.76320,0.76380,0.76460,0.76280,0.76260,0.76280,0.76380,0.76070,0.76130,0.76300,0.76200,0.76480,0.76240,0.76440,0.76120,0.76240,0.76090,0.76320,0.76380,0.76230,0.76190,0.76470,0.76270,0.75980,0.76200,0.76410,0.76220,0.76150,0.76220,0.75920,0.76260,0.76330,0.76270,0.76070,0.76210,0.76310,0.76090,0.75970,0.76140,0.76160,0.76270,0.76240,0.76080,0.76090,0.76250,0.76030,0.76350,0.76160,0.76110,0.76050,"
     ]
    }
   ],
   "source": [
    "for i in np.array(training_acc):\n",
    "    print(f\"{i:.5f}\",end=',')\n",
    "print(\"\\n\\n\")\n",
    "for i in np.array(testing_acc):\n",
    "    print(f\"{i:.5f}\",end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
