{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c78506",
   "metadata": {},
   "source": [
    "# CLIP zero-shot prediction\n",
    "## Basic import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9fc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from clip import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de8bdc",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b680dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seed\n",
    "# SEED = 1 \n",
    "# NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "# NUM_EPOCHS = 30\n",
    "# EVAL_INTERVAL=1\n",
    "# SAVE_DIR = './log'\n",
    "\n",
    "# # Optimizer\n",
    "# LEARNING_RATE = 1e-1\n",
    "# MOMENTUM = 0.9\n",
    "# STEP=5\n",
    "# GAMMA=0.5\n",
    "\n",
    "# CLIP\n",
    "VISUAL_BACKBONE = 'RN50' # RN50, ViT-B/32, ViT-B/16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de39dbc",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f64ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f4c8e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec5f15",
   "metadata": {},
   "source": [
    "### Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b390704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "model, preprocess= clip.load(name=VISUAL_BACKBONE, device=device, download_root='/clip/')\n",
    "model.to(device)\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f690324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集路径\n",
    "data_root = 'food-101/food-101/'\n",
    "\n",
    "# 定义测试集的图像变换操作\n",
    "transform_food101_test = transforms.Compose([\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# 创建测试数据集\n",
    "test_set = torchvision.datasets.ImageFolder(root=data_root + 'test', transform=transform_food101_test)\n",
    "\n",
    "# 创建测试数据加载器\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# 定义类别名称\n",
    "class_names = test_set.classes\n",
    "\n",
    "# 定义数据集名称\n",
    "dataset_name = 'Food-101'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d26b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'a photo of a' # you can try different prompt\n",
    "\n",
    "def prompt_encode(prompt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prompt (str): the text prefix before the class\n",
    "\n",
    "    Returns:\n",
    "        text_inputs(torch.Tensor)\n",
    "\n",
    "    \"\"\"\n",
    "    ##################### Write your answer here ##################\n",
    "    # 将文本编码为tensor    \n",
    "    text_inputs= torch.cat([clip.tokenize(f\"{prompt} {c}\") for c in class_names]).to(device)\n",
    "    ###############################################################\n",
    "    return text_inputs\n",
    "\n",
    "# 编码文本提示\n",
    "text_inputs = prompt_encode(prompt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39948c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model, image, text_inputs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (torch.nn.Module): 训练好的机器学习模型\n",
    "        image (torch.Tensor): 输入图像张量\n",
    "        text_inputs (torch.Tensor): 输入文本张量\n",
    "\n",
    "    Returns:\n",
    "        logits (torch.Tensor): 模型的预测logits\n",
    "    \"\"\"\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    logits = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c5faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the zero-shot performance on Food-101 is 69.35%, visual encoder is RN50.\n"
     ]
    }
   ],
   "source": [
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()  \n",
    "    ##################### Write your answer here ##################\n",
    "    correct_predictions = 0\n",
    "    for image, class_id in test_dataloader:\n",
    "        image=image.to(device)\n",
    "        class_id=class_id.to(device)\n",
    "        \n",
    "        logits=model_inference(model,image,text_inputs)\n",
    "        \n",
    "        predictions = logits.argmax(dim=-1)\n",
    "#         print(predictions,class_id)\n",
    "        correct_predictions += (predictions == class_id).sum().item()\n",
    "    \n",
    "    val_acc = correct_predictions / len(test_set)\n",
    "    \n",
    "    ###############################################################\n",
    "\n",
    "    print(f\"the zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
