{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assginment02: CLIP application\n",
    "In this application, we will use CLIP to assess image similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from clip import clip\n",
    "\n",
    "from transformers import AutoProcessor, CLIPModel, AutoImageProcessor, AutoModel\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CLIP model and processor\n",
    "processor_clip = AutoProcessor.from_pretrained(\"/data/lab/Qi Zimo/STA303-Assignment02/models/clip_processor\")\n",
    "model_clip = CLIPModel.from_pretrained(\"/data/lab/Qi Zimo/STA303-Assignment02/models/clip_model\").to(device)\n",
    "#Load DINOv2 model and processor\n",
    "processor_dino = AutoImageProcessor.from_pretrained('/data/lab/Qi Zimo/STA303-Assignment02/models/dino_processor')\n",
    "model_dino = AutoModel.from_pretrained('/data/lab/Qi Zimo/STA303-Assignment02/models/dino_model').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "#Retrieve all filenames\n",
    "images = []\n",
    "for dirpath, dirnames, filenames in os.walk(r'/data/lab/Qi Zimo/data/val2017/'):  \n",
    "    for filename in filenames:  \n",
    "        if filename.endswith('.jpg'):  \n",
    "            images.append(os.path.join(dirpath, filename)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 5003/5003 [08:42<00:00,  9.58image/s]\n"
     ]
    }
   ],
   "source": [
    "#normalizes embeddings and add to the index\n",
    "def add_vector_to_index(embedding, index):\n",
    "    vector = embedding.detach().cpu().numpy()\n",
    "    vector = np.float32(vector)\n",
    "    faiss.normalize_L2(vector)\n",
    "    index.add(vector)\n",
    "\n",
    "def extract_features_clip(image):\n",
    "    \n",
    "        return image_features\n",
    "\n",
    "def extract_features_dino(image):\n",
    "    \n",
    "        return image_features.mean(dim=1)\n",
    "\n",
    "#Create 2 indexes.\n",
    "index_clip = faiss.IndexFlatL2(512)\n",
    "index_dino = faiss.IndexFlatL2(768)\n",
    "\n",
    "#extract features and store features in indexes\n",
    "for image_path in tqdm(images, desc=\"Processing Images\", unit=\"image\"):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_clip(images=img, return_tensors=\"pt\").to(device)\n",
    "        clip_features = model_clip.get_image_features(**inputs)\n",
    "    add_vector_to_index(clip_features,index_clip)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_dino(images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = model_dino(**inputs)\n",
    "        dino_features = outputs.last_hidden_state\n",
    "    add_vector_to_index(dino_features.mean(dim=1),index_dino)\n",
    "\n",
    "#store the indexes locally\n",
    "faiss.write_index(index_clip,\"clip.index\")\n",
    "faiss.write_index(index_dino,\"dino.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for best similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assessing Images: 100%|██████████| 5007/5007 [49:08<00:00,  1.70image/s]  \n"
     ]
    }
   ],
   "source": [
    "def normalizeL2(embeddings):\n",
    "    vector = embeddings.detach().cpu().numpy()\n",
    "    vector = np.float32(vector)\n",
    "    faiss.normalize_L2(vector)\n",
    "    return vector\n",
    "\n",
    "#read the indexes\n",
    "index_clip = faiss.read_index(\"clip.index\")\n",
    "index_dino = faiss.read_index(\"dino.index\")\n",
    "same_num = 0\n",
    "diff_num = 0\n",
    "\n",
    "dino_for_clip = []\n",
    "clip_for_dino = []\n",
    "dino_best = []\n",
    "clip_best = []\n",
    "\n",
    "for image_path in tqdm(images, desc=\"Assessing Images\", unit=\"image\"):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_clip(images=img, return_tensors=\"pt\").to(device)\n",
    "        clip_features = model_clip.get_image_features(**inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_dino(images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = model_dino(**inputs)\n",
    "        dino_features = outputs.last_hidden_state\n",
    "        dino_features = dino_features.mean(dim=1)\n",
    "    \n",
    "    dino_features = normalizeL2(dino_features)\n",
    "    clip_features = normalizeL2(clip_features)\n",
    "    \n",
    "    d_dino,i_dino = index_dino.search(dino_features,3)\n",
    "    d_clip,i_clip = index_clip.search(clip_features,3)\n",
    "    \n",
    "    if (i_dino[0][1] != i_clip[0][1]):\n",
    "        same_num = same_num +1\n",
    "    elif i_dino[0][1] == i_clip[0][1]:\n",
    "        diff_num = diff_num +1\n",
    "        \n",
    "    dino_image_id = i_dino[0][0:3]\n",
    "    clip_image_id = i_clip[0][0:3]\n",
    "\n",
    "    vec_clip_ori = index_clip.reconstruct(int(clip_image_id[0]))\n",
    "    vec_dino_ori = index_dino.reconstruct(int(dino_image_id[0]))\n",
    "\n",
    "    clip_vecs = []\n",
    "    dino_vecs = []\n",
    "    for i in range(3):\n",
    "        clip_vecs.append(np.dot(index_clip.reconstruct(int(clip_image_id[i])),vec_clip_ori))\n",
    "        dino_vecs.append(np.dot(index_dino.reconstruct(int(dino_image_id[i])),vec_dino_ori))\n",
    "        \n",
    "    dino_best.append(dino_vecs[1])\n",
    "    clip_best.append(clip_vecs[1])\n",
    "    dino_for_clip.append(np.dot(index_dino.reconstruct(int(clip_image_id[1])),index_dino.reconstruct(int(clip_image_id[0]))))\n",
    "    clip_for_dino.append(np.dot(index_clip.reconstruct(int(dino_image_id[1])),index_clip.reconstruct(int(dino_image_id[0]))))\n",
    "    \n",
    "    image_dino = []\n",
    "    image_clip = []\n",
    "    for id in dino_image_id:\n",
    "        image_dino.append(Image.open(images[id]))\n",
    "    for id in clip_image_id:\n",
    "        image_clip.append(Image.open(images[id]))\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 4, figsize=(10, 5))  \n",
    "    axs[0,0].imshow(img)  \n",
    "    axs[0,0].set_title('image')  \n",
    "    axs[0,0].axis('off')\n",
    "    for i in range(3):\n",
    "        axs[0,i+1].imshow(image_dino[i])  \n",
    "        axs[0,i+1].set_title('DINO target'+ str(dino_vecs[i]),fontsize=8) \n",
    "        axs[0,i+1].axis('off') \n",
    "\n",
    "    axs[1,0].imshow(img)  \n",
    "    axs[1,0].set_title('image')  \n",
    "    axs[1,0].axis('off')\n",
    "    for i in range(3):\n",
    "        axs[1,i+1].imshow(image_clip[i])  \n",
    "        axs[1,i+1].set_title('CLIP target'+ str(clip_vecs[i]),fontsize=8)\n",
    "        axs[1,i+1].axis('off') \n",
    "            \n",
    "    plt.savefig('results/'+ str(dino_image_id[0])+'result.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_dec_by_dino_mean 0.11313992072977848\n",
      "dino_dec_by_clip_mean 0.07060249328065685\n",
      "dino_mean 0.7181972342561719\n",
      "clip_mean 0.8145475128000788\n",
      "same_rate 0.8480127821050529\n",
      "diff_rate 0.15198721789494707\n"
     ]
    }
   ],
   "source": [
    "clip_dec_by_dino = [a-b for a,b in zip(dino_best,dino_for_clip)]\n",
    "dino_dec_by_clip = [a-b for a,b in zip(clip_best,clip_for_dino)]\n",
    "clip_dec_by_dino_mean = sum(clip_dec_by_dino)/len(clip_dec_by_dino)\n",
    "dino_dec_by_clip_mean = sum(dino_dec_by_clip)/len(dino_dec_by_clip)\n",
    "\n",
    "dino_mean = sum(dino_best) / len(dino_best)\n",
    "clip_mean = sum(clip_best) / len(clip_best)\n",
    "same_rate = same_num/len(images)\n",
    "diff_rate = diff_num/len(images)\n",
    "# dino_for_clip.count(0)\n",
    "print(\"clip_dec_by_dino_mean\",clip_dec_by_dino_mean)\n",
    "print(\"dino_dec_by_clip_mean\",dino_dec_by_clip_mean)\n",
    "print(\"dino_mean\",dino_mean)\n",
    "print(\"clip_mean\",clip_mean)\n",
    "print(\"same_rate\",same_rate)\n",
    "print(\"diff_rate\",diff_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
