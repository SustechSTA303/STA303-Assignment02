{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Assignment 2: CLIP zero-shot prediction\n",
    "## Testing The Ethnic Bias of Clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Basic Imports\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from clip import clip\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Hyperparameters\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "BATCH_SIZE = 64\n",
    "# CLIP\n",
    "VISUAL_BACKBONE = 'ViT-B/16' # RN50, ViT-B/32, ViT-B/16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Device\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <span style=\"color: blue;\">Dataset\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: red;\">Note: the train datas are not used for Clip, they were use for RESNET to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# transform_cifar10_test = transforms.Compose([\n",
    "#     transforms.Resize(size=224),\n",
    "#     transforms.CenterCrop(size=(224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# test_set = torchvision.datasets.CIFAR10(root='/data/lab/data', train=False,\n",
    "#                                        download=True, transform=transform_cifar10_test)\n",
    "# train_set = torchvision.datasets.CIFAR10(root='/data/lab/data', train=True,\n",
    "#                                        download=True, transform=transform_cifar10_test)\n",
    "\n",
    "# # ########################\n",
    "# # test_set = Subset(test_set, range(5000))##5000\n",
    "# train_set = Subset(train_set, range(5000))##5000\n",
    "# # #######################\n",
    "\n",
    "\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "# dataset_name = 'CIFAR10'\n",
    "\n",
    "# data_loader_size = len(test_dataloader.dataset)\n",
    "# print(f\"Total number of samples in the CIFAR10: {data_loader_size}\")\n",
    "\n",
    "# class_less = True # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ## CIFAR100\n",
    "# transform_cifar100_test = transforms.Compose([\n",
    "#     transforms.Resize(size=224),\n",
    "#     transforms.CenterCrop(size=(224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# test_set = torchvision.datasets.CIFAR100(root='/data/lab/data', train=False,\n",
    "#                                        download=True, transform=transform_cifar100_test)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# train_set = torchvision.datasets.CIFAR100(root='/data/lab/data', train=True,\n",
    "#                                        download=True, transform=transform_cifar100_test)\n",
    "# train_set = Subset(train_set, range(5000))##5000\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# class_names = [\n",
    "#     'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle',\n",
    "#     'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle',\n",
    "#     'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "#     'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard',\n",
    "#     'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain',\n",
    "#     'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree',\n",
    "#     'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "#     'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar',\n",
    "#     'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "#     'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
    "# ]\n",
    "\n",
    "# dataset_name = 'CIFAR100'\n",
    "\n",
    "# data_loader_size = len(test_dataloader.dataset)\n",
    "# print(f\"Total number of samples in the CIFAR100: {data_loader_size}\")\n",
    "\n",
    "# class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STL10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ## STL10\n",
    "# transform_cifar100_test = transforms.Compose([\n",
    "#     transforms.Resize(size=224),\n",
    "#     transforms.CenterCrop(size=(224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# test_set = torchvision.datasets.STL10(root='/data/lab/data', split='test',\n",
    "#                                        download=True, transform=transform_cifar100_test)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# train_set = torchvision.datasets.STL10(root='/data/lab/data', split='train',\n",
    "#                                        download=True, transform=transform_cifar100_test)\n",
    "# train_set = Subset(train_set, range(5000))##5000\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "\n",
    "\n",
    "# dataset_name = 'STL10'\n",
    "\n",
    "# data_loader_size = len(test_dataloader.dataset)\n",
    "# print(f\"Total number of samples in the STL10: {data_loader_size}\")\n",
    "\n",
    "# class_less = True # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stanford dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ##stanford dogs\n",
    "# transform_stanford_dogs_test = transforms.Compose([\n",
    "#     transforms.Resize(size=224),\n",
    "#     transforms.CenterCrop(size=224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "\n",
    "# # Replace 'root_directory' with the path to the directory containing the Stanford Dogs dataset\n",
    "# test_set = datasets.ImageFolder(root='/data/lab/data/Images', transform=transform_stanford_dogs_test)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# # List of class names for the Stanford Dogs dataset (based on the breed names)\n",
    "# class_names = test_set.classes\n",
    "\n",
    "# dataset_name = 'Stanford Dogs'\n",
    "\n",
    "# data_loader_size = len(test_dataloader.dataset)\n",
    "# print(f\"Total number of samples in the Stanford Dogs dataset: {data_loader_size}\")\n",
    "\n",
    "# class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: red;\">since stanford dogs has no test/train different sets. It needs to be part by the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ##stanford dogs for RESNET18\n",
    "# transform_stanford_dogs = transforms.Compose([\n",
    "#     transforms.Resize(size=224),\n",
    "#     transforms.CenterCrop(size=224),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# root_directory = '/data/lab/data/Images'\n",
    "# full_dataset = datasets.ImageFolder(root=root_directory, transform=transform_stanford_dogs)\n",
    "\n",
    "# # Define the ratio of train and test split\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 1 - train_ratio\n",
    "\n",
    "# # Calculate the sizes of train and test sets\n",
    "# train_size = int(train_ratio * len(full_dataset))\n",
    "# test_size = len(full_dataset) - train_size\n",
    "\n",
    "# # Split the dataset into train and test sets\n",
    "# train_set, test_set = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# # Create DataLoaders for train and test sets\n",
    "# train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "# test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# # List of class names for the Stanford Dogs dataset (based on the breed names)\n",
    "# class_names = full_dataset.classes\n",
    "\n",
    "# num_classes = len(class_names)\n",
    "\n",
    "# dataset_name = 'Stanford Dogs'\n",
    "\n",
    "# data_loader_size = len(test_dataloader.dataset)\n",
    "# print(f\"Total number of samples in the Stanford Dogs dataset: {data_loader_size}\")\n",
    "\n",
    "# class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# transform_fashionmnist_test = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=3),\n",
    "#     transforms.Resize(size=224),\n",
    "#     transforms.CenterCrop(size=(224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))  # Use FashionMNIST mean and std\n",
    "\n",
    "# ])\n",
    "\n",
    "# test_set = torchvision.datasets.FashionMNIST(root='/data/lab/data', \n",
    "#                                              download=True, \n",
    "#                                              train=False,  # Set train=False for the test set\n",
    "#                                              transform=transform_fashionmnist_test)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_set, \n",
    "#                                               batch_size=BATCH_SIZE,\n",
    "#                                               shuffle=False, \n",
    "#                                               num_workers=2)\n",
    "# train_set = torchvision.datasets.FashionMNIST(root='/data/lab/data', train=True,\n",
    "#                                        download=True, transform=transform_fashionmnist_test)\n",
    "# train_set = Subset(train_set, range(5000))##5000\n",
    "\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# dataset_name = 'FashionMNIST'\n",
    "\n",
    "# data_loader_size = len(test_dataloader.dataset)\n",
    "# print(f\"Total number of samples in the FashionMNIST test set: {data_loader_size}\")\n",
    "\n",
    "# class_less = len(class_names) <= 10  # Check if the number of classes is 10 or less\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTKFace-ethnic research\n",
    "#### <span style=\"color: red;\">note: the change of ethnicity groups has to be by hand, change the input path last number from 0 to 4 accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age_1', 'age_10', 'age_11', 'age_12', 'age_13', 'age_14', 'age_15', 'age_16', 'age_17', 'age_18', 'age_19', 'age_2', 'age_20', 'age_21', 'age_22', 'age_23', 'age_24', 'age_25', 'age_26', 'age_27', 'age_28', 'age_29', 'age_3', 'age_30', 'age_31', 'age_32', 'age_33', 'age_34', 'age_35', 'age_36', 'age_37', 'age_38', 'age_39', 'age_4', 'age_40', 'age_41', 'age_42', 'age_43', 'age_44', 'age_45', 'age_46', 'age_47', 'age_48', 'age_49', 'age_5', 'age_50', 'age_51', 'age_52', 'age_53', 'age_54', 'age_55', 'age_56', 'age_57', 'age_58', 'age_59', 'age_6', 'age_60', 'age_63', 'age_64', 'age_65', 'age_66', 'age_69', 'age_7', 'age_75', 'age_76', 'age_8', 'age_80', 'age_82', 'age_84', 'age_85', 'age_9']\n",
      "Total number of samples in the UTKFace dataset: 1715\n"
     ]
    }
   ],
   "source": [
    "##UTKFace for clip\n",
    "transform_stanford_dogs_test = transforms.Compose([\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Replace 'root_directory' with the path to the directory containing the Stanford Dogs dataset\n",
    "test_set = datasets.ImageFolder(root='/data/lab/data/UTKFace_eth_age/ethnicity_4', transform=transform_stanford_dogs_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# List of class names for the Stanford Dogs dataset (based on the breed names)\n",
    "class_names = test_set.classes\n",
    "# class_names = [int(class_name.split('_')[1]) for class_name in class_names]\n",
    "print(class_names)\n",
    "\n",
    "dataset_name = 'UTKFace'\n",
    "\n",
    "data_loader_size = len(test_dataloader.dataset)\n",
    "print(f\"Total number of samples in the UTKFace dataset: {data_loader_size}\")\n",
    "\n",
    "class_less = True # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in the UTKFace dataset: 2400\n"
     ]
    }
   ],
   "source": [
    "##UTKFace for RESNET18\n",
    "transform_stanford_dogs = transforms.Compose([\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "root_directory = '/data/lab/data/UTKFace_folded/e_b_folded_num'\n",
    "full_dataset = datasets.ImageFolder(root=root_directory, transform=transform_stanford_dogs)\n",
    "\n",
    "# Define the ratio of train and test split\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Calculate the sizes of train and test sets\n",
    "train_size = int(train_ratio * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_set, test_set = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for train and test sets\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# List of class names for the Stanford Dogs dataset (based on the breed names)\n",
    "class_names = full_dataset.classes\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "dataset_name = 'UTKFace_age'\n",
    "\n",
    "data_loader_size = len(test_dataloader.dataset)\n",
    "print(f\"Total number of samples in the UTKFace dataset: {data_loader_size}\")\n",
    "\n",
    "class_less = False # mark the amount of class number(less or equal than 10 is marked True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Model\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, preprocess = clip.load(name=VISUAL_BACKBONE, device=device, download_root='/shareddata/clip/')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet18\n",
    "#### including training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1150, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/opt/pytorch/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1150, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "# model = torchvision.models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "\n",
    "# # 替换模型最后的全连接层，假设我们要分类10个类别\n",
    "# model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# # 将模型移至GPU（如果可用）\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# # 定义损失函数和优化器\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # 训练模型\n",
    "# for epoch in range(20):  # 假设训练5个epoch\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_dataloader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         if i % 100 == 99:    # 每100个batch输出一次损失\n",
    "#             print(f'Epoch [{epoch + 1}, {i + 1}], Loss: {running_loss / 100:.4f}')\n",
    "#             running_loss = 0.0\n",
    "\n",
    "# print('Finished Training')\n",
    "\n",
    "# # 保存训练好的模型\n",
    "# torch.save(model.state_dict(), 'resnet_cifar10.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Prompt Gereration\n",
    "---\n",
    "\n",
    "Please denfine a function named ``prompt_encode`` to encode the text using CLIP text encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt = 'a photo of a' # you can try different prompt\n",
    "\n",
    "def prompt_encode(prompt):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prompt (str): the text prefix before the class\n",
    "\n",
    "    Returns:\n",
    "        text_inputs(torch.Tensor)\n",
    "\n",
    "    \"\"\"\n",
    "    #################### Write your answer here ##################\n",
    "# #use in datasets excepts for the remaining\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"{prompt} {c}\" for c in class_names)]).to(device)\n",
    "\n",
    "# #use in FashionMNIST\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"{prompt} {c}, a kind of fashion item\" for c in class_names)]).to(device)\n",
    "    \n",
    "# #use in Stanford dogs\n",
    "#     text_inputs = torch.cat([clip.tokenize(f\"{prompt} {c}, a type of dog.\" for c in class_names)]).to(device)\n",
    "\n",
    "#use in UTKFace\n",
    "    text_inputs = torch.cat([clip.tokenize(f\"{prompt} {c} years old person\" for c in class_names)]).to(device)\n",
    "        ###############################################################\n",
    "    \n",
    "    return text_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: blue;\">Zero-shot inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_inference(model, image, text_inputs):\n",
    "\n",
    "    ##################### Write your answer here ##################\n",
    "    image = image.to(device)  # Assuming 'image' is a single-channel image\n",
    "    \n",
    "    # If the model expects a three-channel input (RGB), expand the single channel to three\n",
    "    if image.shape[1] == 1:\n",
    "        image = image.expand(-1, 3, -1, -1)  # Expand single channel to three channels (RGB)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    ###############################################################\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color: blue;\"> Zero-shot accuracy calculation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip Normal cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# testing_loss = []\n",
    "# testing_acc = []\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# t = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     text_inputs = prompt_encode(prompt)\n",
    "\n",
    "#     ##################### Write your answer here ##################\n",
    "#     text_features = model.encode_text(text_inputs)\n",
    "#     text_features /= text_features.norm(dim = 1, keepdim = True)\n",
    "    \n",
    "#     correct = 0\n",
    "#     val_corrects = 0\n",
    "    \n",
    "#     for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "#         t += 1\n",
    "#         target = target.to(device)\n",
    "#         image = image.to(device)\n",
    "        \n",
    "#         similarity = model_inference(model, image, text_inputs)\n",
    "#         values, indices = similarity.max(-1)\n",
    "#         logits = indices\n",
    "        \n",
    "\n",
    "#         total += target.size(0)\n",
    "#         correct += (logits == target).sum().item()\n",
    "        \n",
    "#     val_acc = correct/total\n",
    "        \n",
    "#      ###############################################################\n",
    "#     print(t)\n",
    "#     print(f\"the zero-shot performance on {dataset_name} is {val_acc*100:.2f}%, visual encoder is {VISUAL_BACKBONE}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip With top-k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ##top-k accuracy, don't use in small classes\n",
    "# top_k = 1  # Change this value to your desired top-k\n",
    "# total = 0\n",
    "\n",
    "# training_loss = []\n",
    "# training_acc = []\n",
    "# testing_loss = []\n",
    "# testing_acc = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     text_inputs = prompt_encode(prompt)\n",
    "\n",
    "#     text_features = model.encode_text(text_inputs)\n",
    "#     text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "#     correct = 0\n",
    "#     correct_topk = 0  # Track correct predictions in top-k\n",
    "    \n",
    "#     for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "#         target = target.to(device)\n",
    "#         image = image.to(device)\n",
    "\n",
    "#         logits = model_inference(model, image, text_inputs)\n",
    "        \n",
    "#         _, pred = logits.topk(top_k, dim=1)  # Get top-k predictions\n",
    "\n",
    "#         total += target.size(0)\n",
    "\n",
    "#         # Check if the target is among the top-k predictions for each sample\n",
    "#         if(not class_less):\n",
    "#             for i in range(target.size(0)):\n",
    "#                 if target[i] in pred[i]:\n",
    "#                     correct_topk += 1\n",
    "\n",
    "#         # Check if the top-1 prediction matches the target\n",
    "#         correct += (pred[:, 0] == target).sum().item()  \n",
    "\n",
    "#     val_acc = correct / total\n",
    "    \n",
    "#     if(not class_less):\n",
    "#         val_topk_acc = correct_topk / total  # Calculate top-k accuracy\n",
    "\n",
    "\n",
    "# with open('performance_results.txt', 'a') as file:\n",
    "#         file.write(f'---------------------------------------------\\n')\n",
    "#         file.write(f'More specific prompt \"a type of dog\"\\n')\n",
    "#         file.write(f\"Dataset: {dataset_name}\\n\")\n",
    "#         file.write(f\"Zero-shot performance: {val_acc * 100:.2f}%\\n\")\n",
    "#         if(not class_less):\n",
    "#             file.write(f\"Top-{top_k} accuracy: {val_topk_acc * 100:.2f}%\\n\\n\")\n",
    "        \n",
    "# print(f\"Zero-shot performance on {dataset_name} is {val_acc * 100:.2f}%\")\n",
    "# if(not class_less):\n",
    "#     print(f\"Top-{top_k} accuracy on {dataset_name} is {val_topk_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip for UTKFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "top_k = 1  # Change this value to your desired top-k\n",
    "total = 0\n",
    "\n",
    "# Function to classify age into 15-year intervals\n",
    "def classify_age(age):\n",
    "    return (age // 20) * 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    text_inputs = prompt_encode(prompt)\n",
    "\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    text_features /= text_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    correct = 0\n",
    "    age_group_correct = {}  # Track correct predictions within age groups\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "        target = target.to(device)\n",
    "        image = image.to(device)\n",
    "\n",
    "        logits = model_inference(model, image, text_inputs)\n",
    "        _, pred = logits.topk(top_k, dim=1)  # Remove this line if not needed\n",
    "\n",
    "        total += target.size(0)\n",
    "\n",
    "        correct += (abs(pred[:, 0] - target) <= 5).sum().item() # count as correct within 5 years\n",
    "\n",
    "    val_acc = correct / total\n",
    "\n",
    "    # Calculate accuracy within age groups\n",
    "    age_group_acc = {age_group: age_group_correct.get(age_group, 0) / total for age_group in age_group_correct}\n",
    "\n",
    "with open('ethnicity_age_results.txt', 'a') as file:\n",
    "    file.write(f\"-------------------ViT-B/16----------------------############\\n\")\n",
    "    file.write(f\"age range = 6\\n\")\n",
    "    file.write(f\"ethnicity = All\\n\")\n",
    "    file.write(f\"Dataset: {dataset_name}\\n\")\n",
    "    file.write(f\"Datasize: {data_loader_size}\\n\")\n",
    "    file.write(f\"Zero-shot performance: {val_acc * 100:.2f}%\\n\")\n",
    "        \n",
    "print(f\"Zero-shot performance on {dataset_name} is {val_acc * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##top-k accuracy, don't use in small classes\n",
    "top_k = 5  # Change this value to your desired top-k\n",
    "total = 0\n",
    "\n",
    "\n",
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    text_inputs = prompt_encode(prompt)\n",
    "\n",
    "#     correct = 0\n",
    "#     correct_topk = 0  # Track correct predictions in top-k\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    \n",
    "    for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "        target = target.to(device)\n",
    "        image = image.to(device)\n",
    "        \n",
    "##########################\n",
    "        output = model(image)\n",
    "        \n",
    "        _, preds = torch.max(output, 1)\n",
    "#         print(preds)\n",
    "#         print(target.data)\n",
    "            \n",
    "        val_corrects += torch.sum(abs(preds - target.data) <= 5)\n",
    "\n",
    "    val_acc = val_corrects.double() / len(test_set)\n",
    "    print(f'Acc: {val_acc:.4f}')\n",
    "##########################\n",
    "\n",
    "with open('RESNET18_ethnic.txt', 'a') as file:\n",
    "        file.write(f'---------------------------------------------\\n')\n",
    "        file.write(f\"Dataset: {dataset_name}\\n\")\n",
    "        file.write(f\"ethnicity: White\\n\")\n",
    "        file.write(f\"Zero-shot performance: {val_acc * 100:.2f}%\\n\")\n",
    "        \n",
    "#         if(not class_less):\n",
    "#             file.write(f\"Top-{top_k} accuracy: {val_topk_acc * 100:.2f}%\\n\\n\")\n",
    "        \n",
    "print(f\"Zero-shot performance on {dataset_name} is {val_acc * 100:.2f}%\")\n",
    "# if(not class_less):\n",
    "#     print(f\"Top-{top_k} accuracy on {dataset_name} is {val_topk_acc * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
