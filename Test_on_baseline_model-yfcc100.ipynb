{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057719ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9315a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seed\n",
    "# SEED = 1 \n",
    "# NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 300\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a4862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class yfcc100ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.classes = set()\n",
    "\n",
    "        for folder_name in os.listdir(root_dir):\n",
    "            train_folder_path = os.path.join(root_dir, folder_name, 'train')\n",
    "            test_folder_path = os.path.join(root_dir, folder_name, 'test')\n",
    "\n",
    "            # 检查是否同时存在 train 和 test 子文件夹\n",
    "            if os.path.isdir(train_folder_path) and os.path.isdir(test_folder_path):\n",
    "                if train:\n",
    "                    images_folder_path = os.path.join(train_folder_path, 'images/')\n",
    "                else:\n",
    "                    images_folder_path = os.path.join(test_folder_path, 'images/')\n",
    "\n",
    "                folder_name_modified = re.sub(r'\\d+$', '', folder_name.replace('_', ' '))\n",
    "\n",
    "                if os.path.isdir(images_folder_path):\n",
    "                    for image_name in os.listdir(images_folder_path):\n",
    "                        image_path = os.path.join(images_folder_path, image_name)\n",
    "\n",
    "                        # 检查是否为文件且不是隐藏文件或目录\n",
    "                        if os.path.isfile(image_path) and not image_name.startswith('.') and image_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                            self.samples.append((image_path, folder_name_modified.strip()))\n",
    "                            self.classes.add(self.samples[-1][1])\n",
    "\n",
    "        self.classes = list(self.classes)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples) \n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        image_path, caption = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB') # 确保图片格式为RGB\n",
    "        if self.transform: \n",
    "            image = self.transform(image) \n",
    "        return image, self.classes.index(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ae310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=224),\n",
    "    transforms.CenterCrop(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "train_set=yfcc100ImageDataset(root_dir='/data/lab/STA303-Exercise03/data/yfcc100/OANet/yfcc100m', train=True,transform=transform) \n",
    "test_set=yfcc100ImageDataset(root_dir='/data/lab/STA303-Exercise03/data/yfcc100/OANet/yfcc100m', train=False,transform=transform)\n",
    "train_dataloader=DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_dataloader=DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5333688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "train_set.classes: ['temple nara japan', 'natural history museum london', 'statue of liberty', 'sistine chapel ceiling', 'old town square prague clock', 'big ben', 'taj mahal', 'florence cathedral side', 'palace of westminster', 'mount rushmore', 'piazza dei miracoli', 'national gallery london', 'brandenburg gate', 'grand central terminal new york', 'trevi fountain', 'st vitus cathedral', 'pantheon interior', 'pieta michelangelo', 'grand place brussels', 'petra jordan', 'st pauls cathedral', 'old town square prague', 'blue mosque interior', 'united states capitol rotunda', 'taj mahal entrance', 'pantheon exterior', 'palace of versailles chapel', 'notre dame rosary window', 'colosseum exterior', 'vatican museum ceiling', 'florence cathedral dome interior', 'lincoln memorial', 'louvre', 'lincoln memorial statue', 'westminster abbey', 'london bridge', 'paris opera', 'palazzo pubblico', 'st peters basilica interior', 'temple kyoto japan', 'colosseum interior', 'st peters square', 'hagia sophia interior', 'piazza san marco', 'united states capitol', 'british museum', 'pike place market', 'piazza della signoria', 'ruins of st pauls', 'sagrada familia', 'via condotti', 'milan cathedral', 'western wall jerusalem']\n",
      "53\n",
      "test_set.classes: ['temple nara japan', 'natural history museum london', 'statue of liberty', 'sistine chapel ceiling', 'old town square prague clock', 'big ben', 'taj mahal', 'florence cathedral side', 'palace of westminster', 'mount rushmore', 'piazza dei miracoli', 'national gallery london', 'brandenburg gate', 'grand central terminal new york', 'trevi fountain', 'st vitus cathedral', 'pantheon interior', 'pieta michelangelo', 'grand place brussels', 'petra jordan', 'st pauls cathedral', 'old town square prague', 'blue mosque interior', 'united states capitol rotunda', 'taj mahal entrance', 'pantheon exterior', 'palace of versailles chapel', 'notre dame rosary window', 'colosseum exterior', 'vatican museum ceiling', 'florence cathedral dome interior', 'lincoln memorial', 'louvre', 'lincoln memorial statue', 'westminster abbey', 'london bridge', 'paris opera', 'palazzo pubblico', 'st peters basilica interior', 'temple kyoto japan', 'colosseum interior', 'st peters square', 'hagia sophia interior', 'piazza san marco', 'united states capitol', 'british museum', 'pike place market', 'piazza della signoria', 'ruins of st pauls', 'sagrada familia', 'via condotti', 'milan cathedral', 'western wall jerusalem']\n",
      "Elements only in train_set: set()\n",
      "Elements only in test_set: set()\n"
     ]
    }
   ],
   "source": [
    "class_descriptions = train_set.classes \n",
    "print(len(train_set.classes))\n",
    "print(f\"train_set.classes: {train_set.classes}\")\n",
    "print(len(test_set.classes))\n",
    "print(f\"test_set.classes: {test_set.classes}\")\n",
    "\n",
    "# 假设 train_set.classes 和 test_set.classes 已经定义并且是两个列表\n",
    "\n",
    "# 将列表转换为集合\n",
    "train_classes_set = set(train_set.classes)\n",
    "test_classes_set = set(test_set.classes)\n",
    "\n",
    "# 找出只在 train_set 中的元素\n",
    "only_in_train = train_classes_set - test_classes_set\n",
    "\n",
    "# 找出只在 test_set 中的元素\n",
    "only_in_test = test_classes_set - train_classes_set\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Elements only in train_set: {only_in_train}\")\n",
    "print(f\"Elements only in test_set: {only_in_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2d2b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 53)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu3(self.bn3(self.conv3(x))))\n",
    "\n",
    "        x = x.view(-1, 128 * 28 * 28)\n",
    "\n",
    "        x = self.relu4(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba2369e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (relu4): ReLU()\n",
       "  (fc2): Linear(in_features=512, out_features=53, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cpu\"\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d8f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6edc9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c848215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(model, image, target):\n",
    "\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "        \n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a1c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300 Train Loss: 0.0239 Acc: 0.2463 F1 Score: 0.3500\n",
      "Test Loss: 0.0195 Acc: 0.3810\n",
      "Epoch: 2/300 Train Loss: 0.0182 Acc: 0.4168 F1 Score: 0.4927\n",
      "Test Loss: 0.0163 Acc: 0.4978\n",
      "Epoch: 3/300 Train Loss: 0.0155 Acc: 0.5043 F1 Score: 0.5798\n",
      "Test Loss: 0.0141 Acc: 0.5757\n",
      "Epoch: 4/300 Train Loss: 0.0137 Acc: 0.5618 F1 Score: 0.6364\n",
      "Test Loss: 0.0127 Acc: 0.6170\n",
      "Epoch: 5/300 Train Loss: 0.0124 Acc: 0.6073 F1 Score: 0.6629\n",
      "Test Loss: 0.0118 Acc: 0.6350\n",
      "Epoch: 6/300 Train Loss: 0.0112 Acc: 0.6471 F1 Score: 0.6997\n",
      "Test Loss: 0.0107 Acc: 0.6701\n",
      "Epoch: 7/300 Train Loss: 0.0103 Acc: 0.6714 F1 Score: 0.7295\n",
      "Test Loss: 0.0100 Acc: 0.6998\n",
      "Epoch: 8/300 Train Loss: 0.0096 Acc: 0.6963 F1 Score: 0.7496\n",
      "Test Loss: 0.0095 Acc: 0.7133\n",
      "Epoch: 9/300 Train Loss: 0.0089 Acc: 0.7151 F1 Score: 0.7749\n",
      "Test Loss: 0.0090 Acc: 0.7284\n",
      "Epoch: 10/300 Train Loss: 0.0083 Acc: 0.7356 F1 Score: 0.7930\n",
      "Test Loss: 0.0085 Acc: 0.7365\n",
      "Epoch: 11/300 Train Loss: 0.0078 Acc: 0.7526 F1 Score: 0.8075\n",
      "Test Loss: 0.0083 Acc: 0.7407\n",
      "Epoch: 12/300 Train Loss: 0.0073 Acc: 0.7684 F1 Score: 0.8252\n",
      "Test Loss: 0.0079 Acc: 0.7598\n",
      "Epoch: 13/300 Train Loss: 0.0069 Acc: 0.7820 F1 Score: 0.8403\n",
      "Test Loss: 0.0076 Acc: 0.7660\n",
      "Epoch: 14/300 Train Loss: 0.0065 Acc: 0.7950 F1 Score: 0.8500\n",
      "Test Loss: 0.0073 Acc: 0.7741\n",
      "Epoch: 15/300 Train Loss: 0.0062 Acc: 0.8071 F1 Score: 0.8661\n",
      "Epoch: 16/300 Train Loss: 0.0058 Acc: 0.8186 F1 Score: 0.8745\n",
      "Test Loss: 0.0070 Acc: 0.7858\n",
      "Epoch: 17/300 Train Loss: 0.0055 Acc: 0.8299 F1 Score: 0.8853\n",
      "Test Loss: 0.0067 Acc: 0.7934\n",
      "Epoch: 18/300 Train Loss: 0.0052 Acc: 0.8405 F1 Score: 0.8966\n",
      "Test Loss: 0.0065 Acc: 0.8046\n",
      "Epoch: 19/300 Train Loss: 0.0050 Acc: 0.8468 F1 Score: 0.9034\n",
      "Test Loss: 0.0064 Acc: 0.8062\n",
      "Epoch: 20/300 Train Loss: 0.0047 Acc: 0.8542 F1 Score: 0.9055\n",
      "Test Loss: 0.0063 Acc: 0.8017\n",
      "Epoch: 21/300 Train Loss: 0.0045 Acc: 0.8650 F1 Score: 0.9145\n",
      "Test Loss: 0.0061 Acc: 0.8135\n",
      "Epoch: 22/300 Train Loss: 0.0043 Acc: 0.8723 F1 Score: 0.9207\n",
      "Test Loss: 0.0060 Acc: 0.8159\n",
      "Epoch: 23/300 Train Loss: 0.0041 Acc: 0.8760 F1 Score: 0.9295\n",
      "Test Loss: 0.0058 Acc: 0.8179\n",
      "Epoch: 24/300 Train Loss: 0.0039 Acc: 0.8869 F1 Score: 0.9354\n",
      "Test Loss: 0.0058 Acc: 0.8223\n",
      "Epoch: 25/300 Train Loss: 0.0037 Acc: 0.8929 F1 Score: 0.9387\n",
      "Epoch: 26/300 Train Loss: 0.0036 Acc: 0.8952 F1 Score: 0.9473\n",
      "Test Loss: 0.0056 Acc: 0.8282\n",
      "Epoch: 27/300 Train Loss: 0.0034 Acc: 0.9014 F1 Score: 0.9468\n",
      "Test Loss: 0.0055 Acc: 0.8252\n",
      "Epoch: 28/300 Train Loss: 0.0033 Acc: 0.9061 F1 Score: 0.9551\n",
      "Test Loss: 0.0054 Acc: 0.8323\n",
      "Epoch: 30/300 Train Loss: 0.0029 Acc: 0.9183 F1 Score: 0.9593\n",
      "Test Loss: 0.0053 Acc: 0.8326\n",
      "Epoch: 31/300 Train Loss: 0.0028 Acc: 0.9223 F1 Score: 0.9651\n",
      "Test Loss: 0.0053 Acc: 0.8376\n",
      "Epoch: 32/300 Train Loss: 0.0027 Acc: 0.9262 F1 Score: 0.9666\n",
      "Test Loss: 0.0052 Acc: 0.8405\n",
      "Epoch: 33/300 Train Loss: 0.0026 Acc: 0.9308 F1 Score: 0.9695\n",
      "Test Loss: 0.0051 Acc: 0.8373\n",
      "Epoch: 34/300 Train Loss: 0.0025 Acc: 0.9342 F1 Score: 0.9731\n",
      "Test Loss: 0.0050 Acc: 0.8416\n",
      "Epoch: 35/300 Train Loss: 0.0024 Acc: 0.9362 F1 Score: 0.9742\n",
      "Test Loss: 0.0050 Acc: 0.8432\n",
      "Epoch: 37/300 Train Loss: 0.0022 Acc: 0.9441 F1 Score: 0.9793\n",
      "Test Loss: 0.0049 Acc: 0.8459\n",
      "Epoch: 38/300 Train Loss: 0.0021 Acc: 0.9482 F1 Score: 0.9799\n",
      "Test Loss: 0.0049 Acc: 0.8432\n",
      "Epoch: 39/300 Train Loss: 0.0020 Acc: 0.9499 F1 Score: 0.9824\n",
      "Test Loss: 0.0048 Acc: 0.8446\n",
      "Epoch: 40/300 Train Loss: 0.0020 Acc: 0.9540 F1 Score: 0.9840\n",
      "Test Loss: 0.0047 Acc: 0.8473\n",
      "Epoch: 41/300 Train Loss: 0.0019 Acc: 0.9555 F1 Score: 0.9850\n",
      "Test Loss: 0.0048 Acc: 0.8486\n",
      "Epoch: 42/300 Train Loss: 0.0018 Acc: 0.9565 F1 Score: 0.9866\n",
      "Test Loss: 0.0048 Acc: 0.8501\n",
      "Epoch: 43/300 Train Loss: 0.0017 Acc: 0.9625 F1 Score: 0.9867\n",
      "Test Loss: 0.0047 Acc: 0.8495\n",
      "Epoch: 44/300 Train Loss: 0.0017 Acc: 0.9609 F1 Score: 0.9879\n",
      "Test Loss: 0.0047 Acc: 0.8494\n",
      "Epoch: 45/300 Train Loss: 0.0017 Acc: 0.9627 F1 Score: 0.9890\n",
      "Test Loss: 0.0047 Acc: 0.8479\n",
      "Epoch: 46/300 Train Loss: 0.0015 Acc: 0.9668 F1 Score: 0.9899\n",
      "Test Loss: 0.0047 Acc: 0.8483\n",
      "Epoch: 47/300 Train Loss: 0.0015 Acc: 0.9677 F1 Score: 0.9912\n",
      "Test Loss: 0.0045 Acc: 0.8565\n",
      "Epoch: 48/300 Train Loss: 0.0014 Acc: 0.9700 F1 Score: 0.9918\n",
      "Test Loss: 0.0046 Acc: 0.8552\n",
      "Epoch: 49/300 Train Loss: 0.0014 Acc: 0.9711 F1 Score: 0.9919\n",
      "Test Loss: 0.0047 Acc: 0.8510\n",
      "Epoch: 50/300 Train Loss: 0.0013 Acc: 0.9724 F1 Score: 0.9929\n",
      "Test Loss: 0.0046 Acc: 0.8534\n",
      "Epoch: 51/300 Train Loss: 0.0013 Acc: 0.9746 F1 Score: 0.9926\n",
      "Test Loss: 0.0045 Acc: 0.8541\n",
      "Epoch: 52/300 Train Loss: 0.0013 Acc: 0.9758 F1 Score: 0.9938\n",
      "Test Loss: 0.0045 Acc: 0.8534\n",
      "Epoch: 53/300 Train Loss: 0.0012 Acc: 0.9764 F1 Score: 0.9945\n",
      "Test Loss: 0.0045 Acc: 0.8556\n",
      "Epoch: 54/300 Train Loss: 0.0012 Acc: 0.9794 F1 Score: 0.9946\n",
      "Test Loss: 0.0044 Acc: 0.8585\n",
      "Epoch: 55/300 Train Loss: 0.0011 Acc: 0.9796 F1 Score: 0.9952\n",
      "Test Loss: 0.0044 Acc: 0.8606\n",
      "Epoch: 56/300 Train Loss: 0.0011 Acc: 0.9797 F1 Score: 0.9958\n",
      "Test Loss: 0.0044 Acc: 0.8595\n",
      "Epoch: 57/300 Train Loss: 0.0011 Acc: 0.9823 F1 Score: 0.9956\n",
      "Test Loss: 0.0045 Acc: 0.8534\n",
      "Epoch: 58/300 Train Loss: 0.0010 Acc: 0.9821 F1 Score: 0.9959\n",
      "Test Loss: 0.0045 Acc: 0.8564\n",
      "Epoch: 59/300 Train Loss: 0.0010 Acc: 0.9812 F1 Score: 0.9963\n",
      "Test Loss: 0.0044 Acc: 0.8613\n",
      "Epoch: 60/300 Train Loss: 0.0010 Acc: 0.9852 F1 Score: 0.9965\n",
      "Test Loss: 0.0043 Acc: 0.8620\n",
      "Epoch: 61/300 Train Loss: 0.0010 Acc: 0.9844 F1 Score: 0.9967\n",
      "Test Loss: 0.0043 Acc: 0.8616\n",
      "Epoch: 62/300 Train Loss: 0.0009 Acc: 0.9856 F1 Score: 0.9966\n",
      "Test Loss: 0.0044 Acc: 0.8595\n",
      "Epoch: 63/300 Train Loss: 0.0009 Acc: 0.9860 F1 Score: 0.9968\n",
      "Test Loss: 0.0044 Acc: 0.8568\n",
      "Epoch: 64/300 Train Loss: 0.0009 Acc: 0.9871 F1 Score: 0.9975\n",
      "Test Loss: 0.0043 Acc: 0.8626\n",
      "Epoch: 65/300 Train Loss: 0.0009 Acc: 0.9873 F1 Score: 0.9975\n",
      "Test Loss: 0.0043 Acc: 0.8629\n",
      "Epoch: 66/300 Train Loss: 0.0008 Acc: 0.9873 F1 Score: 0.9974\n",
      "Test Loss: 0.0043 Acc: 0.8610\n",
      "Epoch: 67/300 Train Loss: 0.0008 Acc: 0.9877 F1 Score: 0.9982\n",
      "Test Loss: 0.0043 Acc: 0.8612\n",
      "Epoch: 68/300 Train Loss: 0.0008 Acc: 0.9891 F1 Score: 0.9982\n",
      "Test Loss: 0.0043 Acc: 0.8597\n",
      "Epoch: 69/300 Train Loss: 0.0008 Acc: 0.9903 F1 Score: 0.9979\n",
      "Test Loss: 0.0043 Acc: 0.8631\n",
      "Epoch: 70/300 Train Loss: 0.0007 Acc: 0.9898 F1 Score: 0.9983\n",
      "Test Loss: 0.0042 Acc: 0.8629\n",
      "Epoch: 71/300 Train Loss: 0.0007 Acc: 0.9906 F1 Score: 0.9985\n",
      "Test Loss: 0.0042 Acc: 0.8647\n",
      "Epoch: 72/300 Train Loss: 0.0007 Acc: 0.9915 F1 Score: 0.9976\n",
      "Test Loss: 0.0043 Acc: 0.8625\n",
      "Epoch: 73/300 Train Loss: 0.0007 Acc: 0.9917 F1 Score: 0.9990\n",
      "Test Loss: 0.0042 Acc: 0.8649\n",
      "Epoch: 74/300 Train Loss: 0.0007 Acc: 0.9920 F1 Score: 0.9990\n",
      "Test Loss: 0.0042 Acc: 0.8640\n",
      "Epoch: 75/300 Train Loss: 0.0006 Acc: 0.9929 F1 Score: 0.9988\n",
      "Test Loss: 0.0042 Acc: 0.8664\n",
      "Epoch: 76/300 Train Loss: 0.0006 Acc: 0.9924 F1 Score: 0.9989\n",
      "Test Loss: 0.0042 Acc: 0.8653\n",
      "Epoch: 77/300 Train Loss: 0.0006 Acc: 0.9931 F1 Score: 0.9990\n",
      "Test Loss: 0.0043 Acc: 0.8638\n",
      "Epoch: 78/300 Train Loss: 0.0006 Acc: 0.9933 F1 Score: 0.9992\n",
      "Test Loss: 0.0042 Acc: 0.8668\n",
      "Epoch: 79/300 Train Loss: 0.0006 Acc: 0.9935 F1 Score: 0.9991\n",
      "Test Loss: 0.0042 Acc: 0.8658\n",
      "Epoch: 80/300 Train Loss: 0.0006 Acc: 0.9933 F1 Score: 0.9993\n",
      "Test Loss: 0.0042 Acc: 0.8670\n"
     ]
    }
   ],
   "source": [
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "start_time = time.time()\n",
    "\n",
    "# 日志文件\n",
    "log_file = 'training_log.txt'\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "        #print(f\"for epoch{epoch},batch_idx{batch_idx}, label in train is: {target}\")\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "    # 在每个epoch结束时计算F1分数\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, target in train_dataloader:\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs = model(image)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1 Score: {f1:.4f}')\n",
    "\n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "            #print(f\"for epoch{epoch},batch_idx{batch_idx}, label in test is: {target}\")\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "    # 日志记录\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{NUM_EPOCHS}\\n')\n",
    "        f.write(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1 Score: {f1:.4f}\\n')\n",
    "        f.write(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "    # 模型保存\n",
    "    if (epoch + 1) == NUM_EPOCHS:\n",
    "        state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': epoch + 1\n",
    "        }\n",
    "\n",
    "        if not os.path.exists(SAVE_DIR):\n",
    "            os.makedirs(SAVE_DIR)\n",
    "\n",
    "        torch.save(state, osp.join(SAVE_DIR, f'checkpoint_{epoch+1}.pth'))\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"训练模型用时：{duration}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "start_time = time.time()  # 记录开始时间\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "# 日志文件\n",
    "log_file = 'training_log.txt'\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        #######################\n",
    "        # 为使用L1loss function：\n",
    "       # target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "\n",
    "\n",
    "        # train model\n",
    "       # outputs, loss = train_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "        \n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        #########################\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    #########\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_targets.extend(target.cpu().numpy())\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    print(f\"f1 score in {epoch}th epoch is {f1}\")\n",
    "\n",
    "############\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    \n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            ##################\n",
    "          #  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "          #  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "            #######################\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{NUM_EPOCHS}\\n')\n",
    "        f.write(f'Train Accuracy: {epoch_acc:.4f}%\\n')\n",
    "        f.write(f'Test Accuracy: {val_acc:.4f}%\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # save the model in last epoch\n",
    "        if (epoch +1) == NUM_EPOCHS:\n",
    "            \n",
    "            state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': (epoch+1),\n",
    "            }\n",
    "\n",
    "            # check the dir\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "\n",
    "            # save the state\n",
    "            torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch+1))))\n",
    "            \n",
    "end_time = time.time()  # 记录结束时间\n",
    "duration = end_time - start_time  # 计算训练时间\n",
    "print(f\"训练模型用时：{duration}秒\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
